{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler\n",
    "from scenes_dataset import ScenesDataset, DatasetConstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64 # num of scenes in batch\n",
    "\n",
    "# Scene hyperparams\n",
    "N = 20 # num of nodes\n",
    "\n",
    "# RGCN hyperparams\n",
    "C = 300 # dim of node features\n",
    "R = 23+1 # num of relations\n",
    "\n",
    "# Attention hyperparams\n",
    "D = 315 # dim of objects from the scene\n",
    "\n",
    "hparams = {\n",
    "    'batch_size': B, # num of graphs in batch\n",
    "    \n",
    "    # --- Attention hyperparams ---\n",
    "    'attention_out_dim': D,\n",
    "    'attention_num_heads': 5, # must be a divisor of D\n",
    "    \n",
    "    # --- Encoder RGCN hyperparams ---\n",
    "    'encoder_out_dim': C,\n",
    "    'encoder_hidden_dims': f\"{()}\", # (C, C),\n",
    "    'encoder_num_bases': None,\n",
    "    'encoder_aggr': 'mean',\n",
    "    'encoder_activation': 'tanh',\n",
    "    'encoder_dp_rate': 0.,\n",
    "    'encoder_bias': True,\n",
    "    \n",
    "    # --- Fusion RGCN hyperparams ---\n",
    "    'fusion_hidden_dims': f\"{()}\", # (C+D, C+D, D),\n",
    "    'fusion_num_bases': None,\n",
    "    'fusion_aggr': 'mean',\n",
    "    'fusion_activation': 'tanh',\n",
    "    'fusion_dp_rate': 0.,\n",
    "    'fusion_bias': True,\n",
    "    \n",
    "    # Scheduler hyperparams\n",
    "    'scheduler_timesteps': 1000,\n",
    "    'scheduler_loss': 'l1',\n",
    "    'scheduler_beta_schedule': 'cosine',\n",
    "    # Note: not needed for now\n",
    "    # 'scheduler_sampling_timesteps': None,\n",
    "    # \"scheduler_objective\": 'pred_noise',\n",
    "    # 'scheduler_ddim_sampling_eta': 1.0,\n",
    "    # 'scheduler_min_snr_loss_weight': False,\n",
    "    # 'scheduler_min_snr_gamma': 5,\n",
    "    \n",
    "    # Classifier-free guidance parameters\n",
    "    'cfg_cond_drop_prob': 0.1,\n",
    "    \n",
    "    # Training and optimizer hyperparams\n",
    "    'epochs': 5000,\n",
    "    'optimizer_lr': 1e-3,\n",
    "    'optimizer_weight_decay': 5e-5,\n",
    "    'lr_scheduler_factor': 0.8,\n",
    "    'lr_scheduler_patience': 20,\n",
    "    'lr_scheduler_minlr': 8e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# --- Load the data\n",
    "range_matrix = DatasetConstants.get_range_matrix().to(device)\n",
    "\n",
    "train_dataset = ScenesDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "\n",
    "val_dataset = ScenesDataset(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_N=N,\n",
    "    attention_D=D,\n",
    "    attention_out_dim=hparams['attention_out_dim'],\n",
    "    attention_num_heads=hparams['attention_num_heads'],\n",
    "    \n",
    "    rgcn_num_relations=R,\n",
    "    \n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=hparams['encoder_out_dim'],\n",
    "    encoder_hidden_dims=hparams['encoder_hidden_dims'],\n",
    "    encoder_num_bases=hparams['encoder_num_bases'],\n",
    "    encoder_aggr=hparams['encoder_aggr'],\n",
    "    encoder_activation=hparams['encoder_activation'],\n",
    "    encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "    encoder_bias=hparams['encoder_bias'],\n",
    "    \n",
    "    fusion_hidden_dims=hparams['fusion_hidden_dims'],\n",
    "    fusion_num_bases=hparams['fusion_num_bases'],\n",
    "    fusion_aggr=hparams['fusion_aggr'],\n",
    "    fusion_activation=hparams['fusion_activation'],\n",
    "    fusion_dp_rate=hparams['fusion_dp_rate'],\n",
    "    fusion_bias=hparams['fusion_bias'],\n",
    "    \n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    range_matrix = range_matrix,\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "\n",
    "# --- Setup training loop ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    scheduler.parameters(), \n",
    "    lr=hparams['optimizer_lr'], \n",
    "    weight_decay=hparams['optimizer_weight_decay']\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=hparams['lr_scheduler_factor'], \n",
    "    patience=hparams['lr_scheduler_patience'], \n",
    "    min_lr=hparams['lr_scheduler_minlr']\n",
    ")\n",
    "\n",
    "\n",
    "# --- Initialize tensorboard ---\n",
    "# use timestamp to avoid overwriting previous runs\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter(log_dir=f'runs/full-DDPM/train-time:{now.strftime(\"%Y-%m-%d-%H:%M:%S\")}')\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(hparams['epochs'])):\n",
    "    scheduler.train()\n",
    "    epoch_loss = 0\n",
    "    # --- Training loop ---\n",
    "    for batch in train_dataloader:\n",
    "        x_batch = batch.x.to(device)\n",
    "        obj_cond_batch = batch.cond.to(device)\n",
    "        edge_cond_batch = batch.edge_index.to(device)\n",
    "        relation_cond_batch = batch.edge_attr.to(device)\n",
    "        \n",
    "        # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "        x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "        \n",
    "        loss = scheduler(x_batch, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(train_dataloader)\n",
    "        \n",
    "    lr_scheduler.step(epoch_loss)\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # --- Validation loop ---\n",
    "    with torch.no_grad():\n",
    "        scheduler.eval()\n",
    "        epoch_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            x_batch = batch.x.to(device)\n",
    "            obj_cond_batch = batch.cond.to(device)\n",
    "            edge_cond_batch = batch.edge_index.to(device)\n",
    "            relation_cond_batch = batch.edge_attr.to(device)\n",
    "            \n",
    "            # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "            x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "            \n",
    "            loss = scheduler(x_batch, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(val_dataloader)\n",
    "        writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "        \n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), f'models/best-model.pt')\n",
    "        print(f\"Saved best model with val loss {best_loss}\")\n",
    "    \n",
    "\n",
    "# log all the hyperparameters and final loss\n",
    "writer.add_hparams(hparams, {'Final loss': epoch_loss})\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "def train_scheduler(hparams):\n",
    "    # --- Load the data\n",
    "    dataset = ScenesDataset(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=hparams['batch_size'], shuffle=True, collate_fn=dataset.collate_fn)\n",
    "    \n",
    "    model = GuidedDiffusionNetwork(\n",
    "        attention_in_dim=D,\n",
    "        attention_out_dim=hparams['attention_out_dim'],\n",
    "        attention_num_heads=hparams['attention_num_heads'],\n",
    "        \n",
    "        rgcn_num_relations=R,\n",
    "        \n",
    "        encoder_in_dim=C,\n",
    "        encoder_out_dim=hparams['encoder_out_dim'],\n",
    "        encoder_hidden_dims=hparams['encoder_hidden_dims'],\n",
    "        encoder_num_bases=hparams['encoder_num_bases'],\n",
    "        encoder_aggr=hparams['encoder_aggr'],\n",
    "        encoder_activation=hparams['encoder_activation'],\n",
    "        encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "        encoder_bias=hparams['encoder_bias'],\n",
    "        \n",
    "        fusion_hidden_dims=hparams['fusion_hidden_dims'],\n",
    "        fusion_num_bases=hparams['fusion_num_bases'],\n",
    "        fusion_aggr=hparams['fusion_aggr'],\n",
    "        fusion_activation=hparams['fusion_activation'],\n",
    "        fusion_dp_rate=hparams['fusion_dp_rate'],\n",
    "        fusion_bias=hparams['fusion_bias'],\n",
    "        \n",
    "        cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    "    )\n",
    "\n",
    "    scheduler = DDPMScheduler(\n",
    "        model=model,\n",
    "        N=N,\n",
    "        D=D,\n",
    "        timesteps=hparams['scheduler_timesteps'],\n",
    "        sampling_timesteps=None,\n",
    "        loss_type=hparams['scheduler_loss'],\n",
    "        objective='pred_noise',\n",
    "        beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "        ddim_sampling_eta=1.0,\n",
    "        min_snr_loss_weight=False,\n",
    "        min_snr_gamma=5\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        scheduler.parameters(), \n",
    "        lr=hparams['optimizer_lr'], \n",
    "        weight_decay=hparams['optimizer_weight_decay']\n",
    "    )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=hparams['lr_scheduler_factor'], \n",
    "        patience=hparams['lr_scheduler_patience'], \n",
    "        min_lr=hparams['lr_scheduler_minlr']\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    scheduler = scheduler.to(device)\n",
    "    \n",
    "    # Generate a unique id for each trial\n",
    "    trial_uuid = str(uuid.uuid4())\n",
    "    writer = SummaryWriter(log_dir=f'runs/full-DDPM/hparamtuning-{trial_uuid}')\n",
    "\n",
    "    # --- Training loop ---\n",
    "    best_loss = float('inf')\n",
    "    for epoch in tqdm(range(hparams['epochs'])):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x_batch = batch.x.to(device)\n",
    "            obj_cond_batch = batch.cond.to(device)\n",
    "            edge_cond_batch = batch.edge_index.to(device)\n",
    "            relation_cond_batch = batch.edge_attr.to(device)\n",
    "            \n",
    "            # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "            x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "            \n",
    "            loss = scheduler(x_batch, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(dataloader)\n",
    "        \n",
    "        best_loss = min(best_loss, epoch_loss)\n",
    "            \n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    # log all the hyperparameters and final loss\n",
    "    writer.add_hparams(hparams, {'Best loss': best_loss})\n",
    "    writer.close()\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [4, 16, 32, 64, 128]),\n",
    "    \n",
    "        'attention_out_dim': trial.suggest_categorical('attention_out_dim', [16, 32, 48]),\n",
    "        'attention_num_heads': trial.suggest_categorical('attention_num_heads', [2, 4, 8, 16]),\n",
    "    \n",
    "        'encoder_out_dim': trial.suggest_categorical('encoder_out_dim', [2, 10, 20, 30]),\n",
    "        'encoder_hidden_dims': trial.suggest_categorical('encoder_hidden_dims', [f'{()}', f'{(30,)}', f'{(15,)}', f'{(60,)}', f'{(30, 30)}']),\n",
    "        'encoder_num_bases': trial.suggest_categorical('encoder_num_bases', [None, 2, 4, 8]),\n",
    "        'encoder_aggr': trial.suggest_categorical('encoder_aggr', ['mean', 'sum', 'max']),\n",
    "        'encoder_activation': trial.suggest_categorical('encoder_activation', ['leakyrelu', 'relu', 'silu']),\n",
    "        'encoder_dp_rate': trial.suggest_float('encoder_dp_rate', 0.0, 0.5),\n",
    "        'encoder_bias': trial.suggest_categorical('encoder_bias', [True, False]),\n",
    "    \n",
    "        'fusion_hidden_dims': trial.suggest_categorical('fusion_hidden_dims', [f'{()}', f'{(8,)}', f'{(18,)}', f'{(24,)}', f'{(48,)}', f'{(48, 24)}', f'{(48, 32)}', f\"{(48, 32, 24)}\"]),\n",
    "        'fusion_num_bases': trial.suggest_categorical('fusion_num_bases', [None, 2, 4, 8]),\n",
    "        'fusion_aggr': trial.suggest_categorical('fusion_aggr', ['mean', 'sum', 'max']),\n",
    "        'fusion_activation': trial.suggest_categorical('fusion_activation', ['leakyrelu', 'relu', 'silu']),\n",
    "        'fusion_dp_rate': trial.suggest_float('fusion_dp_rate', 0.0, 0.5),\n",
    "        'fusion_bias': trial.suggest_categorical('fusion_bias', [True, False]),\n",
    "    \n",
    "        'scheduler_timesteps': trial.suggest_categorical('scheduler_timesteps', [1000, 2000, 5000]),\n",
    "        'scheduler_loss': trial.suggest_categorical('scheduler_loss', ['l1', 'l2']),\n",
    "        'scheduler_beta_schedule': trial.suggest_categorical('scheduler_beta_schedule', ['cosine', 'linear']),\n",
    "    \n",
    "        'cfg_cond_drop_prob': trial.suggest_float('cfg_cond_drop_prob', 0.0, 0.5),\n",
    "    \n",
    "        'epochs': 750,\n",
    "        'optimizer_lr': trial.suggest_float('optimizer_lr', 1e-5, 5e-3, log=True),\n",
    "        'optimizer_weight_decay': trial.suggest_float('optimizer_weight_decay', 1e-6, 1e-3, log=True),\n",
    "        'lr_scheduler_factor': trial.suggest_float('lr_scheduler_factor', 0.5, 0.9),\n",
    "        'lr_scheduler_patience': trial.suggest_int('lr_scheduler_patience', 50, 200),\n",
    "        'lr_scheduler_minlr': trial.suggest_float('lr_scheduler_minlr', 0.00001, 0.001, log=True),\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return train_scheduler(hparams)\n",
    "\n",
    "def run_hparam_tuning():\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    return study\n",
    "\n",
    "result_study = run_hparam_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
