{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from attention_layer import ModifiedMultiheadAttention\n",
    "from relational_gcn import RelationalRGCN\n",
    "from time_embedding import TimeEmbedding\n",
    "from ddpm_scheduler import DDPMScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "In this section we put everything together to have a complete diffusion model.\n",
    "\n",
    "The network will go as follows:\n",
    "\n",
    "1. The first part of the network works is the unconditional denoising/diffusion. For that, we represent 3D scenes as a matrix X of dimension [B, N, D] with B batch size, N number of nodes (objects) and D Dimension of each object storing its information (location, size, ...). In this part of the network we feed X through a custom multihead attention layer (see module #1) and get as output data of dimensions [B, N, E] with E hidden Dimension.\n",
    "\n",
    "2. Parallel to the first part, we also receive a scene graph in form of a triple (N, C, C) with N nodes storing string description of objects (\"chair\", \"table\", etc.) and C edges as list of tuples (id_1, id_2) indicating an outgoing connection from node id_1 to node id_2 as well as C connection types as ints. The string description of each node is embedded using FastText (see module #2) and an relational GCN (see module #3) is built with nodes storing these embeddings, and edges as well as edge types extracted from the scene graph following the aforementioned structure. This RGCN block has no hidden layers and outputs dimension F.\n",
    "\n",
    "3. Outputs from the first and second step [B, N, E] and [B, N, F] are concatenated in the last axis to form [B, N, E + F]. Time embedding (see module #4) of matching dimension is produced and added to that result. This matrix is then fed into another relational GCN that consists of N nodes (each row becomes a node) and reuses the edges and edge types from the first RGCN. This new RGCN has a few hidden layers and results in dimension [B, N, D], finishing the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedDiffusionNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Attention block\n",
    "        attention_in_dim,\n",
    "        attention_out_dim,\n",
    "        attention_num_heads, \n",
    "        # Encoder RGCN block\n",
    "        encoder_in_dim, \n",
    "        encoder_out_dim, \n",
    "        encoder_num_relations,\n",
    "        encoder_hidden_dim_list=[],\n",
    "        encoder_num_bases=None,\n",
    "        encoder_aggr='mean',\n",
    "        encoder_activation=nn.LeakyReLU(negative_slope=0.2, inplace=True), # TODO: hyperparam vs. hardcode?\n",
    "        encoder_dp_rate=0.1,\n",
    "        encoder_bias=True,\n",
    "        # Fusion block\n",
    "        fusion_hidden_dim_list=[],\n",
    "        # Classifier-free guidance parameters\n",
    "        cond_drop_prob=0.2,\n",
    "    ):\n",
    "        super(GuidedDiffusionNetwork, self).__init__()\n",
    "        \n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "        \n",
    "        self.attention_module = ModifiedMultiheadAttention(\n",
    "            input_dim=attention_in_dim, \n",
    "            embed_dim=attention_out_dim, \n",
    "            num_heads=attention_num_heads # TODO: hyperparam vs. hardcode?\n",
    "        )\n",
    "        \n",
    "        self.encoder_module = RelationalRGCN(\n",
    "            in_channels=encoder_in_dim, \n",
    "            h_channels_list=encoder_hidden_dim_list,\n",
    "            out_channels=encoder_out_dim,\n",
    "            num_relations=encoder_num_relations, \n",
    "            num_bases=encoder_num_bases, \n",
    "            aggr=encoder_aggr,\n",
    "            activation=encoder_activation,\n",
    "            dp_rate=encoder_dp_rate, \n",
    "            bias=encoder_bias\n",
    "        )\n",
    "        \n",
    "        self.time_embedding_module = TimeEmbedding(dim=attention_out_dim+encoder_out_dim)\n",
    "        \n",
    "        self.fused_rgcn_module = RelationalRGCN(\n",
    "            in_channels=attention_out_dim + encoder_out_dim,\n",
    "            h_channels_list=fusion_hidden_dim_list,\n",
    "            out_channels=attention_in_dim,\n",
    "            num_relations=encoder_num_relations,\n",
    "            num_bases=encoder_num_bases,\n",
    "            # TODO: mirror encoder params?\n",
    "            aggr=encoder_aggr,\n",
    "            activation=encoder_activation,\n",
    "            dp_rate=encoder_dp_rate,\n",
    "            bias=encoder_bias\n",
    "        )\n",
    "    \n",
    "    # This forward method should return the output prediction of noise of the final relational GCN in shape [B, N, D]\n",
    "    def forward(self, x, t, obj_cond, edge_cond, relation_cond, cond_drop_prob=None):\n",
    "        # Step 0: Classifier-free guidance logic\n",
    "        cond_drop_prob = cond_drop_prob if cond_drop_prob is not None else self.cond_drop_prob\n",
    "        # Convert obj_cond to zeros with probability cond_drop_prob\n",
    "        obj_cond = obj_cond * torch.bernoulli(torch.ones_like(obj_cond) * (1 - cond_drop_prob))\n",
    "        \n",
    "        # Step 1: Unconditional denoising/diffusion\n",
    "        x = self.attention_module(x)\n",
    "\n",
    "        # Step 2: Scene graph processing\n",
    "        graph_output = self.encoder_module(obj_cond, edge_cond, relation_cond)\n",
    "        \n",
    "        # Note: instead of stacking [B, N, ...], RGCN uses [B*N, ...] approach, so we need to unstack them to match the shape of x\n",
    "        B, N, _ = x.shape\n",
    "        graph_output = torch.stack(torch.split(graph_output, [N] * B, dim=0), dim=0)\n",
    "\n",
    "        # Step 3: Concatenation and time embedding\n",
    "        fused_output = torch.cat([x, graph_output], dim=-1)\n",
    "        # adapt the time embedding shape ([B, F] -> [B, 1, F]) to use broadcasting when adding to fused_output [B, N, F]\n",
    "        time_embedded = self.time_embedding_module(t)[:, None, :]\n",
    "        fused_output += time_embedded\n",
    "\n",
    "        # Step 4: Final relational GCN\n",
    "        # Note: to feed the data back to RGCN, we need to reshape the data back to [B*N, ...]\n",
    "        output = self.fused_rgcn_module(\n",
    "            fused_output.view(-1, fused_output.size(-1)), \n",
    "            edge_cond, \n",
    "            relation_cond\n",
    "        )\n",
    "        \n",
    "        # Step 5: Reshape the output back to [B, N, ...]\n",
    "        output = output.view(B, N, -1)\n",
    "        return output\n",
    "\n",
    "    # TODO: double check if it makes sense in the future\n",
    "    def forward_with_cond_scale(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale):        \n",
    "        cond_loss = self.forward(x, t, obj_cond, edge_cond, relation_cond, cond_drop_prob=0.)\n",
    "        \n",
    "        if cond_scale == 1:\n",
    "            return cond_loss\n",
    "        \n",
    "        uncond_loss = self.forward(x, t, obj_cond, edge_cond, relation_cond, cond_drop_prob=1.)\n",
    "        \n",
    "        scaled_loss = uncond_loss + (cond_loss - uncond_loss) * cond_scale\n",
    "        # TODO: add rescaled_phi here?\n",
    "        return scaled_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Data\n",
    "\n",
    "In this section, we will generate an example scene graph and a corresponding matrix X to feed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4 # num of graphs in batch\n",
    "\n",
    "N = 20 # num of nodes\n",
    "\n",
    "# RGCN hyperparams\n",
    "C = 6 # dim of node features\n",
    "E = 22 # num of edges\n",
    "R = 8 # num of edge types\n",
    "\n",
    "# Attention hyperparams\n",
    "D = 10 # dim of objects from the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GuidedDiffusionNetwork(\n",
      "  (attention_module): ModifiedMultiheadAttention(\n",
      "    (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "    (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (encoder_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(6, 6, num_relations=8)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (time_embedding_module): TimeEmbedding(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fused_rgcn_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(16, 16, num_relations=8)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): RGCNConv(16, 10, num_relations=8)\n",
      "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): RGCNConv(10, 10, num_relations=8)\n",
      "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=D,\n",
    "    attention_out_dim=D,\n",
    "    attention_num_heads=5,\n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=C,\n",
    "    encoder_num_relations=R,\n",
    "    encoder_hidden_dim_list=[],\n",
    "    encoder_num_bases=None,\n",
    "    encoder_aggr='mean',\n",
    "    encoder_dp_rate=0.1,\n",
    "    encoder_bias=True,\n",
    "    fusion_hidden_dim_list=[C+D, D],\n",
    "    cond_drop_prob=0.2\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4, dimensions of each graph: Data(x=[20, 6], edge_index=[2, 22], edge_attr=[22])\n",
      "tensor([[0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Scene Graphs for conditioning\n",
    "def generate_random_graph(is_one_hot=None):\n",
    "    # --- Initialize nodes ---\n",
    "    if is_one_hot is not None:\n",
    "        nodes = torch.zeros(N, C) # creates N x D tensor of (random) node features\n",
    "        # Initialize nodes with one-hot encoding\n",
    "        for i in range(N):\n",
    "            nodes[i, torch.randint(C, (1,))] = 1\n",
    "    else:\n",
    "        nodes = torch.randn(N, C) # creates N x D tensor of (random) node features\n",
    "    # Initialize nodes with one-hot encoding\n",
    "\n",
    "    # --- Initialize edges --- \n",
    "    edges = torch.randint(N, (2, E)) # creates 2 x E tensor of (random) edges\n",
    "\n",
    "    # --- Introduce different types of edges ---\n",
    "    rels = torch.randint(R, (E,)) # creates E x 1 tensor of (random) edge types\n",
    "\n",
    "    # --- Create a graph ---\n",
    "    graph = Data(x=nodes, edge_index=edges, edge_attr=rels)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# --- Initialize batch ---\n",
    "graphs = [generate_random_graph(is_one_hot=True) for _ in range(B)]\n",
    "\n",
    "print(f\"Batch size: {len(graphs)}, dimensions of each graph: {graphs[0]}\")\n",
    "\n",
    "print(graphs[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: X=torch.Size([4, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "# Scenes for denoising\n",
    "# X = torch.randn(B, N, D) # creates B x N x D tensor of (random) node features\n",
    "# TODO: Create X as ones for testing now\n",
    "Xs = [torch.ones(N, D) * (i / float(B)) for i in range(B)]\n",
    "X = torch.stack(Xs, dim=0)\n",
    "print(f\"Dimensions: X={X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, graphs):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.graphs = graphs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        graph = self.graphs[index]\n",
    "        return {\n",
    "            'x': x,\n",
    "            'obj_cond': graph.x,\n",
    "            'edge_cond': graph.edge_index,\n",
    "            'relation_cond': graph.edge_attr\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x_batch = torch.stack([item['x'] for item in batch], dim=0)\n",
    "        obj_cond_batch = torch.cat([item['obj_cond'] for item in batch], dim=0)\n",
    "        edge_cond_batch = torch.cat([item['edge_cond'] for item in batch], dim=1)\n",
    "        relation_cond_batch = torch.cat([item['relation_cond'] for item in batch], dim=0)\n",
    "\n",
    "        return {\n",
    "            'x': x_batch,\n",
    "            'obj_cond': obj_cond_batch,\n",
    "            'edge_cond': edge_cond_batch,\n",
    "            'relation_cond': relation_cond_batch\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions:\n",
      "\tX=torch.Size([4, 20, 10])\n",
      "\tt=torch.Size([4])\n",
      "\tobj_cond=torch.Size([80, 6])\n",
      "\tedge_cond=torch.Size([2, 88])\n",
      "\trelation_cond=torch.Size([88])\n",
      "Output Dimensions:\n",
      "\toutput=torch.Size([4, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example usage\n",
    "dataset = CustomDataset(X, graphs)\n",
    "dataloader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# Time embedding (should be coming from DDPM scheduler)\n",
    "t = torch.randint(1000, (B,)) # creates B x 1 tensor of (random) time indices\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    x_batch = batch['x']\n",
    "    obj_cond_batch = batch['obj_cond']\n",
    "    edge_cond_batch = batch['edge_cond']\n",
    "    relation_cond_batch = batch['relation_cond']\n",
    "    print(f\"Input Dimensions:\\n\\tX={x_batch.shape}\\n\\tt={t.shape}\\n\\tobj_cond={obj_cond_batch.shape}\\n\\tedge_cond={edge_cond_batch.shape}\\n\\trelation_cond={relation_cond_batch.shape}\")\n",
    "    # Forward pass through the model\n",
    "    output = model(x_batch, t, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "    print(f\"Output Dimensions:\\n\\toutput={output.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the GuidedDiffusionModel to a DDPM Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'B': B, # num of graphs in batch\n",
    "    'N': N, # num of objects in each graph\n",
    "    'D': D, # dim of each object in the scene\n",
    "\n",
    "    # RGCN hyperparams\n",
    "    'C': C, # dim of node features\n",
    "    'E': E, # num of edges\n",
    "    'R': R, # num of edge types\n",
    "    \n",
    "    'encoder_num_bases': None,\n",
    "    'encoder_aggr': 'mean',\n",
    "    'encoder_dp_rate': 0,\n",
    "    'encoder_bias': True,\n",
    "    \n",
    "    # Scheduler hyperparams\n",
    "    'scheduler_timesteps': 1000,\n",
    "    'scheduler_sampling_timesteps': None,\n",
    "    'scheduler_loss': 'l1',\n",
    "    \"scheduler_objective\": 'pred_noise',\n",
    "    'scheduler_beta_schedule': 'cosine',\n",
    "    'scheduler_ddim_sampling_eta': 1.0,\n",
    "    'scheduler_min_snr_loss_weight': False,\n",
    "    'scheduler_min_snr_gamma': 5,\n",
    "    \n",
    "    \n",
    "    # Training and optimizer hyperparams\n",
    "    'epochs': 3000,\n",
    "    'optimizer_lr': 1e-3,\n",
    "    'optimizer_weight_decay': 5e-4,\n",
    "    'lr_scheduler_factor': 0.7,\n",
    "    'lr_scheduler_patience': 100,\n",
    "    'lr_scheduler_minlr': 0.00001,\n",
    "}\n",
    "\n",
    "architecture_hparams = {\n",
    "    # Attention hyperparams\n",
    "    'attention_num_heads': 5,\n",
    "    # Encoder RGCN hyperparams\n",
    "    'encoder_activation': nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "    'encoder_hidden_dim_list': [],\n",
    "    # Fusion RGCN hyperparams\n",
    "    'fusion_hidden_dim_list': [], # [C+D, C+D, D],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "GuidedDiffusionNetwork(\n",
      "  (attention_module): ModifiedMultiheadAttention(\n",
      "    (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "    (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (encoder_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(6, 6, num_relations=8)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (time_embedding_module): TimeEmbedding(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fused_rgcn_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(16, 10, num_relations=8)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "DDPM Scheduler:\n",
      "DDPMScheduler(\n",
      "  (model): GuidedDiffusionNetwork(\n",
      "    (attention_module): ModifiedMultiheadAttention(\n",
      "      (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "      (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "    )\n",
      "    (encoder_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(6, 6, num_relations=8)\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (time_embedding_module): TimeEmbedding(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (fused_rgcn_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(16, 10, num_relations=8)\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:08<00:00, 358.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# --- Load the (mocked) data\n",
    "dataloader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=hparams['D'],\n",
    "    attention_out_dim=hparams['D'],\n",
    "    attention_num_heads=architecture_hparams['attention_num_heads'],\n",
    "    encoder_in_dim=hparams['C'],\n",
    "    encoder_out_dim=hparams['C'],\n",
    "    encoder_num_relations=hparams['R'],\n",
    "    encoder_num_bases=hparams['encoder_num_bases'],\n",
    "    encoder_hidden_dim_list=architecture_hparams['encoder_hidden_dim_list'],\n",
    "    encoder_aggr=hparams['encoder_aggr'],\n",
    "    encoder_activation=architecture_hparams['encoder_activation'],\n",
    "    encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "    encoder_bias=hparams['encoder_bias'],\n",
    "    fusion_hidden_dim_list=architecture_hparams['fusion_hidden_dim_list']\n",
    ")\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=hparams['N'],\n",
    "    D=hparams['D'],\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=hparams['scheduler_sampling_timesteps'],\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective=hparams['scheduler_objective'],\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=hparams['scheduler_ddim_sampling_eta'],\n",
    "    min_snr_loss_weight=hparams['scheduler_min_snr_loss_weight'],\n",
    "    min_snr_gamma=hparams['scheduler_min_snr_gamma']\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "\n",
    "# --- Setup training loop ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    scheduler.parameters(), \n",
    "    lr=hparams['optimizer_lr'], \n",
    "    weight_decay=hparams['optimizer_weight_decay']\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=hparams['lr_scheduler_factor'], \n",
    "    patience=hparams['lr_scheduler_patience'], \n",
    "    min_lr=hparams['lr_scheduler_minlr']\n",
    ")\n",
    "\n",
    "\n",
    "# --- Initialize tensorboard ---\n",
    "# use timestamp to avoid overwriting previous runs\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter(log_dir=f'runs/full-DDPM/overfit-B:{hparams[\"B\"]}-time:{now.strftime(\"%Y-%m-%d-%H:%M:%S\")}')\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in tqdm(range(hparams['epochs'])):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch['x']\n",
    "        obj_cond_batch = batch['obj_cond']\n",
    "        edge_cond_batch = batch['edge_cond']\n",
    "        relation_cond_batch = batch['relation_cond']\n",
    "        \n",
    "        loss = scheduler(x_batch, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(dataloader)\n",
    "        \n",
    "    lr_scheduler.step(epoch_loss)\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "# log all the hyperparameters and final loss\n",
    "writer.add_hparams(hparams, {'Final loss': epoch_loss})\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf08aca41ff419fab6f8d07d0666b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original scene:\n",
      "tensor([[[0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500],\n",
      "         [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500],\n",
      "         [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500],\n",
      "         ...,\n",
      "         [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500],\n",
      "         [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500],\n",
      "         [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "         ...,\n",
      "         [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500,  ..., 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]])\n",
      "Sampled scene:\n",
      "tensor([[[ 1.0000, -0.3013,  1.0000,  ..., -0.2747,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.7580, -0.4429,  ...,  0.4029,  0.8915, -0.0837],\n",
      "         [ 0.7917,  1.0000,  1.0000,  ..., -0.1885,  0.4326,  0.9037],\n",
      "         ...,\n",
      "         [-1.0000, -0.1142, -0.9761,  ..., -0.3960,  0.0672,  0.7960],\n",
      "         [-0.9935, -0.4136, -1.0000,  ..., -1.0000, -1.0000, -0.9080],\n",
      "         [-1.0000, -0.4515, -0.2888,  ..., -0.4451,  0.6058, -0.8919]],\n",
      "\n",
      "        [[-1.0000, -0.1672,  0.0514,  ...,  0.4474, -0.2834, -1.0000],\n",
      "         [ 0.9030, -1.0000,  0.4551,  ..., -0.1912, -0.2690,  0.6048],\n",
      "         [ 0.6095, -1.0000,  0.6789,  ...,  0.8689,  1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  0.7963,  0.9376,  ...,  0.3301,  0.9987, -1.0000],\n",
      "         [-1.0000, -0.7375, -0.7816,  ..., -0.5612, -0.6534, -0.7319],\n",
      "         [ 1.0000, -0.9339, -0.3327,  ...,  0.7033,  0.6261,  0.2502]],\n",
      "\n",
      "        [[-1.0000, -1.0000,  0.1858,  ...,  0.7260, -0.6058, -1.0000],\n",
      "         [-0.4105, -1.0000, -0.2259,  ..., -1.0000, -0.2357, -1.0000],\n",
      "         [-1.0000, -0.8472, -0.9622,  ..., -1.0000, -0.9643,  1.0000],\n",
      "         ...,\n",
      "         [-0.9527, -1.0000, -1.0000,  ...,  0.8647, -0.9414,  0.7175],\n",
      "         [ 0.9972, -1.0000, -0.4730,  ..., -0.3067, -0.1794,  1.0000],\n",
      "         [ 1.0000,  0.7146,  0.7792,  ...,  0.8977,  0.9938,  0.0534]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000, -0.9703,  0.1449,  ..., -0.9941,  0.1241,  0.9250],\n",
      "         [ 0.7350,  1.0000,  0.6359,  ..., -0.2011,  0.8681,  0.9765],\n",
      "         [ 1.0000,  1.0000,  0.9534,  ...,  0.7840,  0.4698, -1.0000],\n",
      "         ...,\n",
      "         [-0.9951,  1.0000,  0.2229,  ...,  0.5890, -1.0000,  0.9339],\n",
      "         [-0.9685,  0.7907, -1.0000,  ..., -0.9948, -0.9797,  0.5408],\n",
      "         [-0.3169, -1.0000,  0.6456,  ..., -1.0000, -0.9562, -0.5660]],\n",
      "\n",
      "        [[ 0.5420,  0.2658,  0.7263,  ..., -0.7391, -0.9996,  0.6708],\n",
      "         [ 0.2853, -1.0000, -0.0242,  ..., -1.0000, -0.4672, -1.0000],\n",
      "         [-0.1457,  0.0260, -0.7193,  ...,  0.2409,  0.2624, -0.9042],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  0.5049,  ...,  0.6411,  0.2222,  1.0000],\n",
      "         [ 1.0000,  0.0651,  0.6832,  ...,  1.0000,  0.7322,  1.0000],\n",
      "         [ 0.0272,  1.0000,  0.2050,  ..., -0.2260, -0.7037,  0.9978]],\n",
      "\n",
      "        [[ 0.8043, -0.1341,  0.9600,  ..., -0.8254,  0.8268, -0.9042],\n",
      "         [ 1.0000, -0.0099,  1.0000,  ...,  0.6907,  0.4157,  0.1622],\n",
      "         [ 1.0000,  1.0000,  0.6804,  ..., -0.4032,  1.0000,  0.9979],\n",
      "         ...,\n",
      "         [ 1.0000,  0.5758,  0.7461,  ..., -0.3262,  0.7161, -0.3134],\n",
      "         [ 0.8861, -0.3558, -0.3368,  ..., -0.0513,  0.4369, -1.0000],\n",
      "         [ 0.8819, -0.9219, -0.9982,  ..., -0.5911, -1.0000, -1.0000]]])\n",
      "MSE: 0.748772919178009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample from the model to see if it works with the same conditioning that was used for overfitting\n",
    "# --- Load the (mocked) data xCOPY copies\n",
    "COPY = 50\n",
    "dataset = CustomDataset(X.repeat(COPY, 1, 1), graphs * COPY)\n",
    "dataloader = DataLoader(dataset, batch_size=B * COPY, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x_batch = batch['x']\n",
    "    obj_cond_batch = batch['obj_cond']\n",
    "    edge_cond_batch = batch['edge_cond']\n",
    "    relation_cond_batch = batch['relation_cond']\n",
    "    \n",
    "    # Sample from the model (use the same conditioning as the overfitting)\n",
    "    output = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=8.0)\n",
    "    \n",
    "    print(f\"Original scene:\\n{x_batch}\")\n",
    "    print(f\"Sampled scene:\\n{output}\")\n",
    "    \n",
    "    # Measure MSE between the original and sampled scenes\n",
    "    mse = torch.nn.functional.mse_loss(x_batch, output)\n",
    "    print(f\"MSE: {mse}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting to a batch of mocked scenes\n",
    "In the following, we have a self-contained example of the network overfitting to a batch of mocked scenes. We will generate a batch of B scenes, each with N objects. The network will be trained to reconstruct the same batch of scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try overfitting the model on a single batch for a reconstruction task\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Reset the model\n",
    "activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=D,\n",
    "    attention_out_dim=D,\n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=C,\n",
    "    encoder_num_relations=R,\n",
    "    encoder_num_bases=None,\n",
    "    encoder_aggr='mean',\n",
    "    encoder_activation=activation,\n",
    "    encoder_dp_rate=0.1,\n",
    "    encoder_bias=True,\n",
    ")\n",
    "\n",
    "# Reset the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# --- Setup training loop ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4 # weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25, min_lr=0.00001)\n",
    "# loss for reconstruction (we use X as the ground truth)\n",
    "recon_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# --- Initialize tensorboard ---\n",
    "# use timestamp to avoid overwriting previous runs\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter(log_dir=f'runs/full-model/overfit-B-{B}-lr-{lr}-time-{now.strftime(\"%Y-%m-%d-%H-%M-%S\")}')\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch['x']\n",
    "        obj_cond_batch = batch['obj_cond']\n",
    "        edge_cond_batch = batch['edge_cond']\n",
    "        relation_cond_batch = batch['relation_cond']\n",
    "        # Forward pass through the model\n",
    "        output = model(x_batch, t, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "        # Compute the loss\n",
    "        loss = recon_loss_fn(output, x_batch)\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    # Log the loss\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step(epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
