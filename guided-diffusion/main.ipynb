{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "In this section we put everything together to have a complete diffusion model.\n",
    "\n",
    "The network will go as follows:\n",
    "\n",
    "TODO: explain the final version\n",
    "\n",
    "<!-- 1. The first part of the network works is the unconditional denoising/diffusion. For that, we represent 3D scenes as a matrix X of dimension [B, N, D] with B batch size, N number of nodes (objects) and D Dimension of each object storing its information (location, size, ...). In this part of the network we feed X through a custom multihead attention layer (see module #1) and get as output data of dimensions [B, N, E] with E hidden Dimension.\n",
    "\n",
    "2. Parallel to the first part, we also receive a scene graph in form of a triple (N, C, C) with N nodes storing string description of objects (\"chair\", \"table\", etc.) and C edges as list of tuples (id_1, id_2) indicating an outgoing connection from node id_1 to node id_2 as well as C connection types as ints. The string description of each node is embedded using FastText (see module #2) and an relational GCN (see module #3) is built with nodes storing these embeddings, and edges as well as edge types extracted from the scene graph following the aforementioned structure. This RGCN block has no hidden layers and outputs dimension F.\n",
    "\n",
    "3. Outputs from the first and second step [B, N, E] and [B, N, F] are concatenated in the last axis to form [B, N, E + F]. Time embedding (see module #4) of matching dimension is produced and added to that result. This matrix is then fed into another relational GCN that consists of N nodes (each row becomes a node) and reuses the edges and edge types from the first RGCN. This new RGCN has a few hidden layers and results in dimension [B, N, D], finishing the forward pass. -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Data\n",
    "\n",
    "In this section, we will generate an example scene graph and a corresponding matrix X to feed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4 # num of graphs in batch\n",
    "\n",
    "N = 20 # num of nodes\n",
    "\n",
    "# RGCN hyperparams\n",
    "C = 6 # dim of node features\n",
    "E = 22 # num of edges\n",
    "R = 8 + 1 # num of edge types (including 'unknown' type)\n",
    "\n",
    "# Attention hyperparams\n",
    "D = 10 # dim of objects from the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GuidedDiffusionNetwork(\n",
      "  (attention_module): ModifiedMultiheadAttention(\n",
      "    (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "    (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (encoder_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(6, 6, num_relations=9)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (time_embedding_module): TimeEmbedding(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fused_rgcn_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(16, 16, num_relations=9)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): RGCNConv(16, 16, num_relations=9)\n",
      "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): RGCNConv(16, 10, num_relations=9)\n",
      "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=D,\n",
    "    attention_out_dim=D,\n",
    "    attention_num_heads=D // 2,\n",
    "    \n",
    "    rgcn_num_relations=R,\n",
    "    \n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=C,\n",
    "    encoder_hidden_dims=f\"{()}\",\n",
    "    encoder_num_bases=None,\n",
    "    encoder_aggr='mean',\n",
    "    encoder_activation=\"leakyrelu\",\n",
    "    encoder_dp_rate=0.1,\n",
    "    encoder_bias=True,\n",
    "    \n",
    "    fusion_hidden_dims=f\"{(C+D, C+D)}\",\n",
    "    fusion_num_bases=None,\n",
    "    fusion_aggr='mean',\n",
    "    fusion_activation=\"leakyrelu\",\n",
    "    fusion_dp_rate=0.1,\n",
    "    fusion_bias=True,\n",
    "    \n",
    "    cond_drop_prob=0.2\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4, dimensions of each graph: Data(x=[20, 6], edge_index=[2, 22], edge_attr=[22])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Scene Graphs for conditioning\n",
    "def generate_random_graph(is_one_hot=None):\n",
    "    # --- Initialize nodes ---\n",
    "    if is_one_hot is not None:\n",
    "        nodes = torch.zeros(N, C) # creates N x D tensor of (random) node features\n",
    "        # Initialize nodes with one-hot encoding\n",
    "        for i in range(N):\n",
    "            nodes[i, torch.randint(C, (1,))] = 1\n",
    "    else:\n",
    "        nodes = torch.randn(N, C) # creates N x D tensor of (random) node features\n",
    "    # Initialize nodes with one-hot encoding\n",
    "\n",
    "    # --- Initialize edges --- \n",
    "    edges = torch.randint(N, (2, E)) # creates 2 x E tensor of (random) edges\n",
    "\n",
    "    # --- Introduce different types of edges ---\n",
    "    rels = torch.randint(R - 1, (E,)) + 1 # creates E x 1 tensor of (random) edge types excluding 'unknown' type with value 0\n",
    "\n",
    "    # --- Create a graph ---\n",
    "    graph = Data(x=nodes, edge_index=edges, edge_attr=rels)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# --- Initialize batch ---\n",
    "graphs = [generate_random_graph(is_one_hot=True) for _ in range(B)]\n",
    "\n",
    "print(f\"Batch size: {len(graphs)}, dimensions of each graph: {graphs[0]}\")\n",
    "\n",
    "# print(graphs[0].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: X=torch.Size([4, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "# Scenes for denoising\n",
    "# X = torch.randn(B, N, D) # creates B x N x D tensor of (random) node features\n",
    "# TODO: Create X as ones for testing now\n",
    "Xs = [torch.ones(N, D) * (i / float(B)) for i in range(B)]\n",
    "X = torch.stack(Xs, dim=0)\n",
    "print(f\"Dimensions: X={X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, graphs):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.graphs = graphs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        graph = self.graphs[index]\n",
    "        return {\n",
    "            'x': x,\n",
    "            'obj_cond': graph.x,\n",
    "            'edge_cond': graph.edge_index,\n",
    "            'relation_cond': graph.edge_attr\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x_batch = torch.stack([item['x'] for item in batch], dim=0)\n",
    "        obj_cond_batch = torch.cat([item['obj_cond'] for item in batch], dim=0)\n",
    "        edge_cond_batch = torch.cat([item['edge_cond'] for item in batch], dim=1)\n",
    "        relation_cond_batch = torch.cat([item['relation_cond'] for item in batch], dim=0)\n",
    "\n",
    "        return {\n",
    "            'x': x_batch,\n",
    "            'obj_cond': obj_cond_batch,\n",
    "            'edge_cond': edge_cond_batch,\n",
    "            'relation_cond': relation_cond_batch\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions:\n",
      "\tX=torch.Size([4, 20, 10])\n",
      "\tt=torch.Size([4])\n",
      "\tobj_cond=torch.Size([80, 6])\n",
      "\tedge_cond=torch.Size([2, 88])\n",
      "\trelation_cond=torch.Size([88])\n",
      "Pass with cond_drop_prob=0.0\n",
      "Pass with cond_drop_prob=1.0\n",
      "Output Dimensions:\n",
      "\toutput=torch.Size([4, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example usage\n",
    "dataset = CustomDataset(X, graphs)\n",
    "dataloader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# Time embedding (should be coming from DDPM scheduler)\n",
    "t = torch.randint(1000, (B,)) # creates B x 1 tensor of (random) time indices\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    x_batch = batch['x']\n",
    "    obj_cond_batch = batch['obj_cond']\n",
    "    edge_cond_batch = batch['edge_cond']\n",
    "    relation_cond_batch = batch['relation_cond']\n",
    "    print(f\"Input Dimensions:\\n\\tX={x_batch.shape}\\n\\tt={t.shape}\\n\\tobj_cond={obj_cond_batch.shape}\\n\\tedge_cond={edge_cond_batch.shape}\\n\\trelation_cond={relation_cond_batch.shape}\")\n",
    "    # Forward pass through the model\n",
    "    print(f\"Pass with cond_drop_prob=0.0\")\n",
    "    output = model(x_batch, t, obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_drop_prob=0.)\n",
    "    print(f\"Pass with cond_drop_prob=1.0\")\n",
    "    output = model(x_batch, t, obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_drop_prob=1.)\n",
    "    print(f\"Output Dimensions:\\n\\toutput={output.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the GuidedDiffusionModel to a DDPM Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'batch_size': B, # num of graphs in batch\n",
    "    \n",
    "    # --- Attention hyperparams ---\n",
    "    'attention_out_dim': D,\n",
    "    'attention_num_heads': 5, # must be a divisor of D\n",
    "    \n",
    "    # --- Encoder RGCN hyperparams ---\n",
    "    'encoder_out_dim': C,\n",
    "    'encoder_hidden_dims': f\"{()}\", # (C, C),\n",
    "    'encoder_num_bases': None,\n",
    "    'encoder_aggr': 'mean',\n",
    "    'encoder_activation': 'leakyrelu',\n",
    "    'encoder_dp_rate': 0.,\n",
    "    'encoder_bias': True,\n",
    "    \n",
    "    # --- Fusion RGCN hyperparams ---\n",
    "    'fusion_hidden_dims': f\"{()}\", # (C+D, C+D, D),\n",
    "    'fusion_num_bases': None,\n",
    "    'fusion_aggr': 'mean',\n",
    "    'fusion_activation': 'leakyrelu',\n",
    "    'fusion_dp_rate': 0.,\n",
    "    'fusion_bias': True,\n",
    "    \n",
    "    # Scheduler hyperparams\n",
    "    'scheduler_timesteps': 1000,\n",
    "    'scheduler_loss': 'l1',\n",
    "    'scheduler_beta_schedule': 'cosine',\n",
    "    # Note: not needed for now\n",
    "    # 'scheduler_sampling_timesteps': None,\n",
    "    # \"scheduler_objective\": 'pred_noise',\n",
    "    # 'scheduler_ddim_sampling_eta': 1.0,\n",
    "    # 'scheduler_min_snr_loss_weight': False,\n",
    "    # 'scheduler_min_snr_gamma': 5,\n",
    "    \n",
    "    # Classifier-free guidance parameters\n",
    "    'cfg_cond_drop_prob': 0.1,\n",
    "    \n",
    "    # Training and optimizer hyperparams\n",
    "    'epochs': 3000,\n",
    "    'optimizer_lr': 1e-3,\n",
    "    'optimizer_weight_decay': 5e-4,\n",
    "    'lr_scheduler_factor': 0.7,\n",
    "    'lr_scheduler_patience': 100,\n",
    "    'lr_scheduler_minlr': 0.00001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "GuidedDiffusionNetwork(\n",
      "  (attention_module): ModifiedMultiheadAttention(\n",
      "    (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "    (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (encoder_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(6, 6, num_relations=9)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (time_embedding_module): TimeEmbedding(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fused_rgcn_module): RelationalRGCN(\n",
      "    (layers): ModuleList(\n",
      "      (0): RGCNConv(16, 10, num_relations=9)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "DDPM Scheduler:\n",
      "DDPMScheduler(\n",
      "  (model): GuidedDiffusionNetwork(\n",
      "    (attention_module): ModifiedMultiheadAttention(\n",
      "      (qkv_proj): Linear(in_features=10, out_features=30, bias=False)\n",
      "      (o_proj): Linear(in_features=10, out_features=10, bias=False)\n",
      "    )\n",
      "    (encoder_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(6, 6, num_relations=9)\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (time_embedding_module): TimeEmbedding(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (fused_rgcn_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(16, 10, num_relations=9)\n",
      "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:08<00:00, 345.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# --- Load the (mocked) data\n",
    "dataloader = DataLoader(dataset, batch_size=hparams['batch_size'], shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=D,\n",
    "    attention_out_dim=hparams['attention_out_dim'],\n",
    "    attention_num_heads=hparams['attention_num_heads'],\n",
    "    \n",
    "    rgcn_num_relations=R,\n",
    "    \n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=hparams['encoder_out_dim'],\n",
    "    encoder_hidden_dims=hparams['encoder_hidden_dims'],\n",
    "    encoder_num_bases=hparams['encoder_num_bases'],\n",
    "    encoder_aggr=hparams['encoder_aggr'],\n",
    "    encoder_activation=hparams['encoder_activation'],\n",
    "    encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "    encoder_bias=hparams['encoder_bias'],\n",
    "    \n",
    "    fusion_hidden_dims=hparams['fusion_hidden_dims'],\n",
    "    fusion_num_bases=hparams['fusion_num_bases'],\n",
    "    fusion_aggr=hparams['fusion_aggr'],\n",
    "    fusion_activation=hparams['fusion_activation'],\n",
    "    fusion_dp_rate=hparams['fusion_dp_rate'],\n",
    "    fusion_bias=hparams['fusion_bias'],\n",
    "    \n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "\n",
    "# --- Setup training loop ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    scheduler.parameters(), \n",
    "    lr=hparams['optimizer_lr'], \n",
    "    weight_decay=hparams['optimizer_weight_decay']\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=hparams['lr_scheduler_factor'], \n",
    "    patience=hparams['lr_scheduler_patience'], \n",
    "    min_lr=hparams['lr_scheduler_minlr']\n",
    ")\n",
    "\n",
    "\n",
    "# --- Initialize tensorboard ---\n",
    "# use timestamp to avoid overwriting previous runs\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "writer = SummaryWriter(log_dir=f'runs/full-DDPM/overfit-time:{now.strftime(\"%Y-%m-%d-%H:%M:%S\")}')\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in tqdm(range(hparams['epochs'])):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch['x'].to(device)\n",
    "        obj_cond_batch = batch['obj_cond'].to(device)\n",
    "        edge_cond_batch = batch['edge_cond'].to(device)\n",
    "        relation_cond_batch = batch['relation_cond'].to(device)\n",
    "        \n",
    "        loss = scheduler(x_batch, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(dataloader)\n",
    "        \n",
    "    lr_scheduler.step(epoch_loss)\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "# log all the hyperparameters and final loss\n",
    "writer.add_hparams(hparams, {'Final loss': epoch_loss})\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ec9384dcd444a499efaff0dd81c32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m relation_cond_batch \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mrelation_cond\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[39m# Sample from the model (use the same conditioning as the overfitting)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m output \u001b[39m=\u001b[39m scheduler\u001b[39m.\u001b[39;49msample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale\u001b[39m=\u001b[39;49m\u001b[39m8.0\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal scene:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mx_batch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSampled scene:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:295\u001b[0m, in \u001b[0;36mDDPMScheduler.sample\u001b[0;34m(self, obj_cond, edge_cond, relation_cond, cond_scale)\u001b[0m\n\u001b[1;32m    293\u001b[0m batch_size, N, D \u001b[39m=\u001b[39m obj_cond\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD\n\u001b[1;32m    294\u001b[0m sample_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_sample_loop \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_ddim_sampling \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddim_sample\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m sample_fn(obj_cond, edge_cond, relation_cond, (batch_size, N, D), cond_scale)\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:231\u001b[0m, in \u001b[0;36mDDPMScheduler.p_sample_loop\u001b[0;34m(self, obj_cond, edge_cond, relation_cond, shape, cond_scale)\u001b[0m\n\u001b[1;32m    228\u001b[0m x_start \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tqdm(\u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)), desc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msampling loop time step\u001b[39m\u001b[39m'\u001b[39m, total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps):\n\u001b[0;32m--> 231\u001b[0m     data, x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(data, t, obj_cond, edge_cond, relation_cond, cond_scale)\n\u001b[1;32m    233\u001b[0m \u001b[39m# data = unnormalize_to_zero_to_one(data) # NEEDED?????\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:209\u001b[0m, in \u001b[0;36mDDPMScheduler.p_sample\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    207\u001b[0m b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    208\u001b[0m batched_times \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), t, device \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdevice, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong) \u001b[39m# vector of length dim0(x) filled with t\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m model_mean, _, model_log_variance, x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(x \u001b[39m=\u001b[39;49m x, t \u001b[39m=\u001b[39;49m batched_times, obj_cond \u001b[39m=\u001b[39;49m obj_cond, edge_cond \u001b[39m=\u001b[39;49m edge_cond, relation_cond \u001b[39m=\u001b[39;49m relation_cond, cond_scale \u001b[39m=\u001b[39;49m cond_scale, clip_denoised \u001b[39m=\u001b[39;49m clip_denoised)\n\u001b[1;32m    210\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x) \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0.\u001b[39m \u001b[39m# no noise if t == 0\u001b[39;00m\n\u001b[1;32m    211\u001b[0m pred_img \u001b[39m=\u001b[39m model_mean \u001b[39m+\u001b[39m (\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m model_log_variance)\u001b[39m.\u001b[39mexp() \u001b[39m*\u001b[39m noise\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:190\u001b[0m, in \u001b[0;36mDDPMScheduler.p_mean_variance\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\u001b[39mself\u001b[39m, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    Given x_t, t and graph_cond (obj_cond [BxNxC], edge_cond [Bx2xE], relation_cond [BxE])\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    --> gets x_0 from 'model_predictions'\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    --> sends x_0, x_t, t to 'y_posterior' to recover mean and variance of p(x_t-1 | x_0, x_t, t)\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m    --> outputs mü, var, log_var, x_0\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_predictions(x, t, obj_cond, edge_cond, relation_cond, cond_scale) \u001b[39m# preds = ('ModelPrediction', ['pred_noise', 'pred_x_start'])\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     x_start \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mpred_x_start\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m clip_denoised:\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:155\u001b[0m, in \u001b[0;36mDDPMScheduler.model_predictions\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_x_start)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_predictions\u001b[39m(\u001b[39mself\u001b[39m, x, t, obj_cond, edge_cond, relation_cond, cond_scale \u001b[39m=\u001b[39m \u001b[39m3.\u001b[39m, clip_x_start \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    150\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m    Given x_t [BxNxD], t [BxNxD] and graph_cond (obj_cond [BxNxC], edge_cond [Bx2xE], relation_cond [BxE])\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m    --> forward pass through NN\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    --> outputs eps_theta(x_t, t) and x0\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, t, obj_cond, edge_cond, relation_cond, cond_scale \u001b[39m=\u001b[39;49m cond_scale) \u001b[39m# forward block im RGCN\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m#maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \n\u001b[1;32m    159\u001b[0m     \u001b[39m# dynamic thresholding instead\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpred_noise\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m# I assume we use this one\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/model.py:177\u001b[0m, in \u001b[0;36mGuidedDiffusionNetwork.forward_with_cond_scale\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m cond_scale \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m cond_loss\n\u001b[0;32m--> 177\u001b[0m uncond_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, t, obj_cond, edge_cond, relation_cond, cond_drop_prob\u001b[39m=\u001b[39;49m\u001b[39m1.\u001b[39;49m)\n\u001b[1;32m    179\u001b[0m scaled_loss \u001b[39m=\u001b[39m uncond_loss \u001b[39m+\u001b[39m (cond_loss \u001b[39m-\u001b[39m uncond_loss) \u001b[39m*\u001b[39m cond_scale\n\u001b[1;32m    180\u001b[0m \u001b[39m# TODO: add rescaled_phi here?\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/model.py:145\u001b[0m, in \u001b[0;36mGuidedDiffusionNetwork.forward\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_drop_prob)\u001b[0m\n\u001b[1;32m    140\u001b[0m fused_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time_embedded\n\u001b[1;32m    143\u001b[0m \u001b[39m# --- Step 4: Final relational GCN\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# Note: to feed the data back to RGCN, we need to reshape the data back to [B*N, ...]\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfused_rgcn_module(\n\u001b[1;32m    146\u001b[0m     fused_output\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, fused_output\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \n\u001b[1;32m    147\u001b[0m     edge_cond, \n\u001b[1;32m    148\u001b[0m     relation_cond\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    152\u001b[0m \u001b[39m# --- Step 5: Reshape the output back to [B, N, ...]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(B, N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/relational_gcn.py:83\u001b[0m, in \u001b[0;36mRelationalRGCN.forward\u001b[0;34m(self, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, RGCNConv):\n\u001b[0;32m---> 83\u001b[0m         x \u001b[39m=\u001b[39m layer(x, edge_index, edge_type)\n\u001b[1;32m     84\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch_geometric/nn/conv/rgcn_conv.py:257\u001b[0m, in \u001b[0;36mRGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m                 h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(tmp, x\u001b[39m=\u001b[39mx_l, edge_type_ptr\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    256\u001b[0m                                    size\u001b[39m=\u001b[39msize)\n\u001b[0;32m--> 257\u001b[0m                 out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m (h \u001b[39m@\u001b[39m weight[i])\n\u001b[1;32m    259\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m root \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample from the model to see if it works with the same conditioning that was used for overfitting\n",
    "# --- Load the (mocked) data xCOPY copies\n",
    "COPY = 50\n",
    "dataset = CustomDataset(X.repeat(COPY, 1, 1), graphs * COPY)\n",
    "dataloader = DataLoader(dataset, batch_size=B * COPY, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x_batch = batch['x']\n",
    "    obj_cond_batch = batch['obj_cond']\n",
    "    edge_cond_batch = batch['edge_cond']\n",
    "    relation_cond_batch = batch['relation_cond']\n",
    "    \n",
    "    # Sample from the model (use the same conditioning as the overfitting)\n",
    "    output = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=8.0)\n",
    "    \n",
    "    print(f\"Original scene:\\n{x_batch}\")\n",
    "    print(f\"Sampled scene:\\n{output}\")\n",
    "    \n",
    "    # Measure MSE between the original and sampled scenes\n",
    "    mse = torch.nn.functional.mse_loss(x_batch, output)\n",
    "    print(f\"MSE: {mse}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting to a batch of mocked scenes\n",
    "In the following, we have a self-contained example of the network overfitting to a batch of mocked scenes. We will generate a batch of B scenes, each with N objects. The network will be trained to reconstruct the same batch of scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 292/1000 [00:41<01:41,  6.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Backprop\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     59\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Try overfitting the model on a single batch for a reconstruction task\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # --- Reset the model\n",
    "# model = GuidedDiffusionNetwork(\n",
    "#     attention_in_dim=hparams['D'],\n",
    "#     attention_out_dim=hparams['D'],\n",
    "#     attention_num_heads=architecture_hparams['attention_num_heads'],\n",
    "#     encoder_in_dim=hparams['C'],\n",
    "#     encoder_out_dim=hparams['C'],\n",
    "#     encoder_num_relations=hparams['R'],\n",
    "#     encoder_num_bases=hparams['encoder_num_bases'],\n",
    "#     encoder_hidden_dim_list=architecture_hparams['encoder_hidden_dim_list'],\n",
    "#     encoder_aggr=hparams['encoder_aggr'],\n",
    "#     encoder_activation=architecture_hparams['encoder_activation'],\n",
    "#     encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "#     encoder_bias=hparams['encoder_bias'],\n",
    "#     fusion_hidden_dim_list=architecture_hparams['fusion_hidden_dim_list'],\n",
    "#     cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    "# )\n",
    "\n",
    "# # Reset the dataloader\n",
    "# dataloader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# # --- Setup training loop ---\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# epochs = 1000\n",
    "\n",
    "# lr = 1e-3\n",
    "# weight_decay = 5e-4 # weight decay\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=25, min_lr=0.00001)\n",
    "# # loss for reconstruction (we use X as the ground truth)\n",
    "# recon_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# # --- Initialize tensorboard ---\n",
    "# # use timestamp to avoid overwriting previous runs\n",
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# writer = SummaryWriter(log_dir=f'runs/full-model/overfit-B-{B}-lr-{lr}-time-{now.strftime(\"%Y-%m-%d-%H-%M-%S\")}')\n",
    "\n",
    "# # --- Training loop ---\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     epoch_loss = 0\n",
    "#     for batch in dataloader:\n",
    "#         x_batch = batch['x']\n",
    "#         obj_cond_batch = batch['obj_cond']\n",
    "#         edge_cond_batch = batch['edge_cond']\n",
    "#         relation_cond_batch = batch['relation_cond']\n",
    "#         # Forward pass through the model\n",
    "#         output = model(x_batch, t, obj_cond_batch, edge_cond_batch, relation_cond_batch)\n",
    "#         # Compute the loss\n",
    "#         loss = recon_loss_fn(output, x_batch)\n",
    "#         # Backprop\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     epoch_loss /= len(dataloader)\n",
    "#     # Log the loss\n",
    "#     writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    \n",
    "#     # Update the learning rate\n",
    "#     scheduler.step(epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
