{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from attention_layer import ModifiedMultiheadAttention\n",
    "from encoder_rgcn import EncoderRGCN\n",
    "from text_encoder import FastTextEncoder\n",
    "from time_embedding import TimeEmbedding\n",
    "from ddpm_scheduler import DDPMScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "In this section we put everything together to have a complete diffusion model.\n",
    "\n",
    "The network will go as follows:\n",
    "\n",
    "1. The first part of the network works is the unconditional denoising/diffusion. For that, we represent 3D scenes as a matrix X of dimension [B, N, D] with B batch size, N number of nodes (objects) and D dimension of each object storing its information (location, size, ...). In this part of the network we feed X through a custom multihead attention layer (see module #1) and get as output data of dimensions [B, N, E] with E hidden dimension.\n",
    "\n",
    "2. Parallel to the first part, we also receive a scene graph in form of a triple (N, C, C) with N nodes storing string description of objects (\"chair\", \"table\", etc.) and C edges as list of tuples (id_1, id_2) indicating an outgoing connection from node id_1 to node id_2 as well as C connection types as ints. The string description of each node is embedded using FastText (see module #2) and an relational GCN (see module #3) is built with nodes storing these embeddings, and edges as well as edge types extracted from the scene graph following the aforementioned structure. This RGCN block has no hidden layers and outputs dimension F.\n",
    "\n",
    "3. Outputs from the first and second step [B, N, E] and [B, N, F] are concatenated in the last axis to form [B, N, E + F]. Time embedding (see module #4) of matching dimension is produced and added to that result. This matrix is then fed into another relational GCN that consists of N nodes (each row becomes a node) and reuses the edges and edge types from the first RGCN. This new RGCN has a few hidden layers and results in dimension [B, N, D], finishing the forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedDiffusionNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Attention block\n",
    "        attention_in_dim, \n",
    "        attention_out_dim, \n",
    "        # Encoder RGCN block\n",
    "        encoder_in_dim, \n",
    "        encoder_out_dim, \n",
    "        encoder_num_relations,\n",
    "        encoder_num_bases=None,\n",
    "        encodeer_aggr='mean',\n",
    "        encoder_dp_rate=0.1,\n",
    "        encoder_bias=True,\n",
    "    ):\n",
    "        super(GuidedDiffusionNetwork, self).__init__()\n",
    "        \n",
    "        self.attention_module = ModifiedMultiheadAttention(\n",
    "            input_dim=attention_in_dim, \n",
    "            target_dim=attention_out_dim, \n",
    "            num_heads=attention_in_dim // 4 # TODO: hyperparam vs. hardcode?\n",
    "        )\n",
    "        \n",
    "        self.encoder_module = EncoderRGCN(\n",
    "            in_channels=encoder_in_dim, \n",
    "            h_channels_list = [], # TODO: no hidden layers?\n",
    "            out_channels = encoder_out_dim,\n",
    "            num_relations=encoder_num_relations, \n",
    "            num_bases=encoder_num_bases, \n",
    "            aggr=encodeer_aggr, \n",
    "            dp_rate=encoder_dp_rate, \n",
    "            bias=encoder_bias\n",
    "        )\n",
    "        \n",
    "        self.time_embedding_module = TimeEmbedding(dim=attention_out_dim+encoder_out_dim)\n",
    "    \n",
    "    # TODO: adapt the forward method to match DDPM scheduler: model.forward(x, t, obj_cond, edge_cond, relation_cond)\n",
    "    def forward(self, x, scene_graph, time):\n",
    "        # Step 1: Unconditional denoising/diffusion\n",
    "        x = self.attention_module(x)\n",
    "\n",
    "        # Step 2: Scene graph processing\n",
    "        embedded_graph = torch.stack([FastTextEncoder.encode(word) for word in scene_graph])\n",
    "        embedded_graph = embedded_graph.squeeze(1)\n",
    "        graph_output = self.encoder_module(embedded_graph, edge_index, edge_type)\n",
    "\n",
    "        # Step 3: Concatenation and time embedding\n",
    "        fused_output = torch.cat([x, graph_output], dim=-1)\n",
    "        time_embedded = self.time_embedding_module(time)\n",
    "        fused_output += time_embedded\n",
    "\n",
    "        # TODO: Step 4: Final relational GCN\n",
    "        output = fused_output # TODO: placeholder\n",
    "\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
