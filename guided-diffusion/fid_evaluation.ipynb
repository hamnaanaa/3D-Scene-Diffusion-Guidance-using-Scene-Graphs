{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_fid import fid_score\n",
    "from pytorch_fid import fid_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "explain me + installation of pytorch-fid and run with python -m pytorch_fid path/to/fake_images path/to/real_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenes_dataset import DatasetConstants\n",
    "\n",
    "def normalize_to_0_1(scene_matrix):\n",
    "    \"\"\"\n",
    "    Normalize the scene matrix to the range [0, 1].\n",
    "    \"\"\"\n",
    "    range_matrix = DatasetConstants.get_range_matrix()\n",
    "    # TODO: these values are not 100%-percentile but 99%-percentile min/max values\n",
    "    max_value, min_value = range_matrix[0], range_matrix[1]\n",
    "    normalized_scene_matrix = (scene_matrix - min_value) / (max_value - min_value)\n",
    "    return normalized_scene_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth dataset preparation\n",
    "\n",
    "Map every scene to its scene matrix, normalize it between 0 and 1, treat as an L image and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']\n",
    "    \n",
    "gt_data = train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "for scene in gt_data:\n",
    "    scene_matrix = torch.tensor(scene[\"scene_matrix\"], dtype=torch.float32)\n",
    "    scene_matrix = normalize_to_0_1(scene_matrix)\n",
    "    # print raw data\n",
    "    # print(f\"Min: {scene_matrix.min()}, Max: {scene_matrix.max()}\")\n",
    "    scene_img = Image.fromarray(scene_matrix.numpy(), mode=\"L\")\n",
    "    scene_img.save(f\"fid_data/gt/{scene['scene_id']}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic dataset generation and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler\n",
    "from scenes_dataset import ScenesDataset, DatasetConstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32 # num of scenes in batch\n",
    "\n",
    "# Scene hyperparams\n",
    "N = 20 # num of objects in scene\n",
    "D = 15 # dim of objects from the scene\n",
    "\n",
    "# Time hyperparams\n",
    "T = 14\n",
    "\n",
    "# Condition hyperparmas\n",
    "C = 300 # dim of node features\n",
    "R = 23+1 # num of relations\n",
    "\n",
    "hparams = {\n",
    "    # constants\n",
    "    'epochs': 2000, 'scheduler_loss': 'l2', 'rgc_activation': 'tanh',\n",
    "    # from hparam search\n",
    "    'batch_size': B, 'time_dim': 44, 'rgc_hidden_dims': '()', 'rgc_num_bases': 4, 'rgc_aggr': 'mean', 'rgc_dp_rate': 0.14463856683812687, 'rgc_bias': False, 'attention_self_head_dims': 30, 'attention_num_heads': 1, 'attention_cross_head_dims': 30, 'scheduler_timesteps': 1000, 'scheduler_beta_schedule': 'linear', 'cfg_cond_drop_prob': 0.16303181894889107, 'optimizer_lr': 0.000571096217369203, 'optimizer_weight_decay': 0.00010261093147577781, 'lr_scheduler_factor': 0.813888153675873, 'lr_scheduler_patience': 60, 'lr_scheduler_minlr': 0.00036368282361166394\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = {\n",
    "    \"num_obj\": N,\n",
    "    \"obj_cond_dim\": C,\n",
    "    'layer_1_dim': D,\n",
    "    'layer_2_dim': D + hparams['time_dim'],\n",
    "    \"time_dim\": hparams['time_dim'],\n",
    "}\n",
    "\n",
    "attention_params = {\n",
    "    \"attention_self_head_dim\": hparams['attention_self_head_dims'],\n",
    "    \"attention_num_heads\": hparams['attention_num_heads'],\n",
    "    \"attention_cross_head_dim\": hparams['attention_cross_head_dims']\n",
    "}\n",
    "\n",
    "rgc_params = {\n",
    "    \"rgc_hidden_dims\": hparams['rgc_hidden_dims'],\n",
    "    \"rgc_num_relations\": R,\n",
    "    \"rgc_num_bases\": hparams['rgc_num_bases'],\n",
    "    \"rgc_aggr\": hparams['rgc_aggr'],\n",
    "    \"rgc_activation\": hparams['rgc_activation'],\n",
    "    \"rgc_dp_rate\": hparams['rgc_dp_rate'],\n",
    "    \"rgc_bias\": hparams['rgc_bias']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# --- Load the data\n",
    "range_matrix = DatasetConstants.get_range_matrix().to(device)\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    general_params=general_params,\n",
    "    attention_params=attention_params,\n",
    "    rgc_params=rgc_params,\n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "# load the best model\n",
    "model_name = \"models/val-model_0146_l2_all+CFG.pt\"\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "print(f\"Loaded model from {model_name}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    range_matrix = range_matrix,\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "model.eval()\n",
    "scheduler.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_dataset = ScenesDataset(train_data)\n",
    "val_dataset = ScenesDataset(val_data)\n",
    "\n",
    "all_dataloader = DataLoader(train_dataset + val_dataset, batch_size=B, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for batch in all_dataloader:\n",
    "    # x_batch = batch.x.to(device)\n",
    "    obj_cond_batch = batch.cond.to(device)\n",
    "    edge_cond_batch = batch.edge_index.to(device)\n",
    "    relation_cond_batch = batch.edge_attr.to(device)\n",
    "    \n",
    "    # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "    # x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "    # obj_cond is read as [B*N, C] and needs to be reshaped to [B, N, C]\n",
    "    obj_cond_batch = obj_cond_batch.view(batch.num_graphs, N, C)\n",
    "    \n",
    "    labels_batch = batch.labels\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():      \n",
    "        # Sample from the model with conditions matching train validation data\n",
    "        sampled_scenes = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=3., return_all_samples=False)\n",
    "        for i in range(sampled_scenes.shape[0]):\n",
    "            # Save the sampled scene to a JSON file\n",
    "            scene_matrix = normalize_to_0_1(sampled_scenes[i])\n",
    "            # print raw data\n",
    "            # print(f\"Min: {scene_matrix.min()}, Max: {scene_matrix.max()}\")\n",
    "            scene_img = Image.fromarray(scene_matrix.numpy(), mode=\"L\")\n",
    "            scene_img.save(f\"fid_data/synth_l2_double_cfg_3/{counter}.png\")\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "B = 400\n",
    "N = 20\n",
    "D = 15\n",
    "\n",
    "X1 = torch.randn(B, N, D)\n",
    "X2 = torch.randn(B, N, D)\n",
    "\n",
    "for i in range(B):\n",
    "    # Treat each NxD matrix as a single image and save these images to disk\n",
    "    x1_img = Image.fromarray(X1[i].numpy(), mode=\"L\")\n",
    "    x2_img = Image.fromarray(X2[i].numpy(), mode=\"L\")\n",
    "    x1_img.save(f\"tmp/gt/X1_{i}.png\")\n",
    "    x2_img.save(f\"tmp/synth/X2_{i}.png\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# for i in range(B):\n",
    "#     torch.save(X1[i], f\"tmp/gt/X1_{i}.pt\")\n",
    "#     torch.save(X2[i], f\"tmp/synth/X2_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
