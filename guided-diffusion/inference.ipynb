{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model_alternative import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler\n",
    "from scenes_dataset import ScenesDataset, DatasetConstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, scenes):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.scenes = scenes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene = self.scenes[index]\n",
    "        \n",
    "        scene_matrix = torch.tensor(scene[\"scene_matrix\"], dtype=torch.float32)\n",
    "        graph_objects = torch.tensor(scene[\"graph_objects\"], dtype=torch.float32)\n",
    "        graph_edges = torch.tensor(scene[\"graph_edges\"], dtype=torch.long)\n",
    "        graph_relationships = torch.tensor(scene[\"graph_relationships\"], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'x': scene_matrix,\n",
    "            'obj_cond': graph_objects,\n",
    "            'edge_cond': graph_edges,\n",
    "            'relation_cond': graph_relationships\n",
    "        }\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x_batch = torch.stack([item['x'] for item in batch], dim=0)\n",
    "        obj_cond_batch = torch.cat([item['obj_cond'] for item in batch], dim=0)\n",
    "        edge_cond_batch = torch.cat([item['edge_cond'] for item in batch], dim=1)\n",
    "        relation_cond_batch = torch.cat([item['relation_cond'] for item in batch], dim=0)\n",
    "\n",
    "        return {\n",
    "            'x': x_batch,\n",
    "            'obj_cond': obj_cond_batch,\n",
    "            'edge_cond': edge_cond_batch,\n",
    "            'relation_cond': relation_cond_batch\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']\n",
    "  \n",
    "# Not available yet  \n",
    "# with open('datasets/data/test.json', 'r') as file:\n",
    "#     test_data = json.load(file)['scenes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 3 # num of scenes in batch\n",
    "\n",
    "# Scene hyperparams\n",
    "N = 20 # num of objects in scene\n",
    "D = 15 # dim of objects from the scene\n",
    "\n",
    "# Condition hyperparmas\n",
    "C = 300 # dim of node features\n",
    "R = 23+1 # num of relations\n",
    "\n",
    "hparams = {\n",
    "    'batch_size': B, # num of graphs in batch\n",
    "    'layer_2_dim': 29, # must be a divisor of 300\n",
    "\n",
    "    # --- RGCN hyperparams ---\n",
    "    'rgc_hidden_dims': f\"{()}\", # (C+D, C+D, D),\n",
    "    'rgc_num_bases': 5, # Alternative: None\n",
    "    'rgc_aggr': 'mean',\n",
    "    'rgc_activation': 'tanh',\n",
    "    'rgc_dp_rate': 0.,\n",
    "    'rgc_bias': True,\n",
    "    \n",
    "    # --- Attention hyperparams ---\n",
    "    'attention_self_head_dims': 10,\n",
    "    'attention_num_heads': 3, \n",
    "    'attention_cross_head_dims': 30,\n",
    "    \n",
    "    # Scheduler hyperparams\n",
    "    'scheduler_timesteps': 1000,\n",
    "    'scheduler_loss': 'l2',\n",
    "    'scheduler_beta_schedule': 'cosine',\n",
    "    # Note: not needed for now\n",
    "    # 'scheduler_sampling_timesteps': None,\n",
    "    # \"scheduler_objective\": 'pred_noise',\n",
    "    # 'scheduler_ddim_sampling_eta': 1.0,\n",
    "    # 'scheduler_min_snr_loss_weight': False,\n",
    "    # 'scheduler_min_snr_gamma': 5,\n",
    "    \n",
    "    # Classifier-free guidance parameters\n",
    "    'cfg_cond_drop_prob': 0.,\n",
    "    \n",
    "    # Training and optimizer hyperparams\n",
    "    'epochs': 2000,\n",
    "    'optimizer_lr': 1e-3,\n",
    "    'optimizer_weight_decay': 5e-5,\n",
    "    'lr_scheduler_factor': 0.8,\n",
    "    'lr_scheduler_patience': 20,\n",
    "    'lr_scheduler_minlr': 8e-5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = {\n",
    "    \"num_obj\": N,\n",
    "    \"obj_cond_dim\": C\n",
    "}\n",
    "\n",
    "attention_params = {\n",
    "    \"attention_self_head_dim\": hparams['attention_self_head_dims'],\n",
    "    \"attention_num_heads\": hparams['attention_num_heads'],\n",
    "    \"attention_cross_head_dim\": hparams['attention_cross_head_dims']\n",
    "}\n",
    "\n",
    "rgc_params = {\n",
    "    \"rgc_hidden_dims\": hparams['rgc_hidden_dims'],\n",
    "    \"rgc_num_relations\": R,\n",
    "    \"rgc_num_bases\": hparams['rgc_num_bases'],\n",
    "    \"rgc_aggr\": hparams['rgc_aggr'],\n",
    "    \"rgc_activation\": hparams['rgc_activation'],\n",
    "    \"rgc_dp_rate\": hparams['rgc_dp_rate'],\n",
    "    \"rgc_bias\": hparams['rgc_bias']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "GuidedDiffusionNetwork(\n",
      "  (block1): GuidedDiffusionBlock(\n",
      "    (time_embedding_module): TimeEmbedding()\n",
      "    (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
      "    (rgc_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(15, 15, num_relations=24)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (self_attention_module): SelfMultiheadAttention(\n",
      "      (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "      (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (cross_attention_module): CrossMultiheadAttention(\n",
      "      (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "      (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "      (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=29, out_features=29, bias=True)\n",
      "  (block2): GuidedDiffusionBlock(\n",
      "    (time_embedding_module): TimeEmbedding()\n",
      "    (max_pool): MaxPool1d(kernel_size=(10,), stride=(10,), padding=0, dilation=1, ceil_mode=False)\n",
      "    (rgc_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(29, 29, num_relations=24)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (self_attention_module): SelfMultiheadAttention(\n",
      "      (qkv_proj): Linear(in_features=29, out_features=90, bias=False)\n",
      "      (o_proj): Linear(in_features=30, out_features=29, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (cross_attention_module): CrossMultiheadAttention(\n",
      "      (q_proj): Linear(in_features=29, out_features=90, bias=False)\n",
      "      (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "      (o_proj): Linear(in_features=90, out_features=29, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (linear2): Linear(in_features=29, out_features=29, bias=True)\n",
      "  (block3): GuidedDiffusionBlock(\n",
      "    (time_embedding_module): TimeEmbedding()\n",
      "    (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
      "    (rgc_module): RelationalRGCN(\n",
      "      (layers): ModuleList(\n",
      "        (0): RGCNConv(15, 15, num_relations=24)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (self_attention_module): SelfMultiheadAttention(\n",
      "      (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "      (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (cross_attention_module): CrossMultiheadAttention(\n",
      "      (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "      (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "      (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
      "      (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (linear3): Linear(in_features=29, out_features=29, bias=True)\n",
      "  (linear4): Linear(in_features=29, out_features=15, bias=True)\n",
      ")\n",
      "DDPM Scheduler:\n",
      "DDPMScheduler(\n",
      "  (model): GuidedDiffusionNetwork(\n",
      "    (block1): GuidedDiffusionBlock(\n",
      "      (time_embedding_module): TimeEmbedding()\n",
      "      (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
      "      (rgc_module): RelationalRGCN(\n",
      "        (layers): ModuleList(\n",
      "          (0): RGCNConv(15, 15, num_relations=24)\n",
      "          (1): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (self_attention_module): SelfMultiheadAttention(\n",
      "        (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "        (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (cross_attention_module): CrossMultiheadAttention(\n",
      "        (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "        (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (linear1): Linear(in_features=29, out_features=29, bias=True)\n",
      "    (block2): GuidedDiffusionBlock(\n",
      "      (time_embedding_module): TimeEmbedding()\n",
      "      (max_pool): MaxPool1d(kernel_size=(10,), stride=(10,), padding=0, dilation=1, ceil_mode=False)\n",
      "      (rgc_module): RelationalRGCN(\n",
      "        (layers): ModuleList(\n",
      "          (0): RGCNConv(29, 29, num_relations=24)\n",
      "          (1): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (self_attention_module): SelfMultiheadAttention(\n",
      "        (qkv_proj): Linear(in_features=29, out_features=90, bias=False)\n",
      "        (o_proj): Linear(in_features=30, out_features=29, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (cross_attention_module): CrossMultiheadAttention(\n",
      "        (q_proj): Linear(in_features=29, out_features=90, bias=False)\n",
      "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "        (o_proj): Linear(in_features=90, out_features=29, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (linear2): Linear(in_features=29, out_features=29, bias=True)\n",
      "    (block3): GuidedDiffusionBlock(\n",
      "      (time_embedding_module): TimeEmbedding()\n",
      "      (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
      "      (rgc_module): RelationalRGCN(\n",
      "        (layers): ModuleList(\n",
      "          (0): RGCNConv(15, 15, num_relations=24)\n",
      "          (1): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (self_attention_module): SelfMultiheadAttention(\n",
      "        (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "        (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (cross_attention_module): CrossMultiheadAttention(\n",
      "        (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
      "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
      "        (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
      "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (linear3): Linear(in_features=29, out_features=29, bias=True)\n",
      "    (linear4): Linear(in_features=29, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DDPMScheduler(\n",
       "  (model): GuidedDiffusionNetwork(\n",
       "    (block1): GuidedDiffusionBlock(\n",
       "      (time_embedding_module): TimeEmbedding()\n",
       "      (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
       "      (rgc_module): RelationalRGCN(\n",
       "        (layers): ModuleList(\n",
       "          (0): RGCNConv(15, 15, num_relations=24)\n",
       "          (1): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (self_attention_module): SelfMultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
       "        (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (cross_attention_module): CrossMultiheadAttention(\n",
       "        (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
       "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
       "        (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear1): Linear(in_features=29, out_features=29, bias=True)\n",
       "    (block2): GuidedDiffusionBlock(\n",
       "      (time_embedding_module): TimeEmbedding()\n",
       "      (max_pool): MaxPool1d(kernel_size=(10,), stride=(10,), padding=0, dilation=1, ceil_mode=False)\n",
       "      (rgc_module): RelationalRGCN(\n",
       "        (layers): ModuleList(\n",
       "          (0): RGCNConv(29, 29, num_relations=24)\n",
       "          (1): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (self_attention_module): SelfMultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=29, out_features=90, bias=False)\n",
       "        (o_proj): Linear(in_features=30, out_features=29, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (cross_attention_module): CrossMultiheadAttention(\n",
       "        (q_proj): Linear(in_features=29, out_features=90, bias=False)\n",
       "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
       "        (o_proj): Linear(in_features=90, out_features=29, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 29), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear2): Linear(in_features=29, out_features=29, bias=True)\n",
       "    (block3): GuidedDiffusionBlock(\n",
       "      (time_embedding_module): TimeEmbedding()\n",
       "      (max_pool): MaxPool1d(kernel_size=(20,), stride=(20,), padding=0, dilation=1, ceil_mode=False)\n",
       "      (rgc_module): RelationalRGCN(\n",
       "        (layers): ModuleList(\n",
       "          (0): RGCNConv(15, 15, num_relations=24)\n",
       "          (1): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (self_attention_module): SelfMultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=15, out_features=90, bias=False)\n",
       "        (o_proj): Linear(in_features=30, out_features=15, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (cross_attention_module): CrossMultiheadAttention(\n",
       "        (q_proj): Linear(in_features=15, out_features=90, bias=False)\n",
       "        (kv_proj): Linear(in_features=300, out_features=180, bias=False)\n",
       "        (o_proj): Linear(in_features=90, out_features=15, bias=False)\n",
       "        (layer_norm): LayerNorm((20, 15), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear3): Linear(in_features=29, out_features=29, bias=True)\n",
       "    (linear4): Linear(in_features=29, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# --- Load the data\n",
    "range_matrix = DatasetConstants.get_range_matrix().to(device)\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    layer_1_dim=D,\n",
    "    layer_2_dim=hparams['layer_2_dim'],\n",
    "    general_params=general_params,\n",
    "    attention_params=attention_params,\n",
    "    rgc_params=rgc_params,\n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/overfit-model.pt'))\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    range_matrix = range_matrix[:, C:],\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "model.eval()\n",
    "scheduler.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ScenesDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_dataset = ScenesDataset(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semseg_file(sampled_scene, scan_id=0):\n",
    "    # Take the first element in the batch and only the last 15 dimensions of it\n",
    "    filtered_scene = sampled_scene[0, :, -15:]\n",
    "\n",
    "    objs = []\n",
    "    for i in range(20):\n",
    "        label = 'unknown' # TODO: generate label from neighborhood search from the embeddings\n",
    "        location = filtered_scene[i, 0:3]\n",
    "        normalized_axes = filtered_scene[i, 3:12]\n",
    "        sizes = filtered_scene[i, 12:15]\n",
    "        \n",
    "        objs.append({\n",
    "            'obb': {\n",
    "                'centroid': location.tolist(),\n",
    "                'normalizedAxes': normalized_axes.tolist(),\n",
    "                'axesLengths': sizes.tolist()\n",
    "            },\n",
    "            'label': label,\n",
    "            'dominantNormal': [0, 0, 0], # not used for now\n",
    "        })\n",
    "\n",
    "    # Store the sampled scene to visualize using DVIS\n",
    "    encoded_scene = {\n",
    "        'scan_id': scan_id,\n",
    "        'segGroups': objs, # TODO: add segGroups\n",
    "    }\n",
    "\n",
    "    # save the sampled scene to a JSON file (create the folder if it doesn't exist)\n",
    "    with open(f'datasets/data/gen/{scan_id}_semseg.v2.json', 'w') as file:\n",
    "        json.dump(encoded_scene, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51b428831434387b3a9434ee89b9a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "quantile() input tensor must be non-empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Run inference\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():      \n\u001b[1;32m     14\u001b[0m     \u001b[39m# Sample from the model (use the same conditioning as the overfitting)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     sampled_scene \u001b[39m=\u001b[39m scheduler\u001b[39m.\u001b[39;49msample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale\u001b[39m=\u001b[39;49m\u001b[39m5.0\u001b[39;49m, return_all_samples\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:301\u001b[0m, in \u001b[0;36mDDPMScheduler.sample\u001b[0;34m(self, obj_cond, edge_cond, relation_cond, cond_scale, return_all_samples)\u001b[0m\n\u001b[1;32m    299\u001b[0m batch_size, N, D \u001b[39m=\u001b[39m obj_cond\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD\n\u001b[1;32m    300\u001b[0m sample_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_sample_loop \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_ddim_sampling \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddim_sample\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m sample_fn(obj_cond, edge_cond, relation_cond, (batch_size, N, D), cond_scale, return_all_samples \u001b[39m=\u001b[39;49m return_all_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:237\u001b[0m, in \u001b[0;36mDDPMScheduler.p_sample_loop\u001b[0;34m(self, obj_cond, edge_cond, relation_cond, shape, cond_scale, return_all_samples)\u001b[0m\n\u001b[1;32m    234\u001b[0m     all_samples \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, DDPMUtils\u001b[39m.\u001b[39munnormalize_to_original(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrange_matrix))]\n\u001b[1;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tqdm(\u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)), desc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msampling loop time step\u001b[39m\u001b[39m'\u001b[39m, total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps):\n\u001b[0;32m--> 237\u001b[0m     data, x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(data, t, obj_cond, edge_cond, relation_cond, cond_scale)\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m return_all_samples:\n\u001b[1;32m    239\u001b[0m         all_samples\u001b[39m.\u001b[39mappend((t, DDPMUtils\u001b[39m.\u001b[39munnormalize_to_original(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrange_matrix)))\n",
      "File \u001b[0;32m~/miniconda3/envs/adl4cv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:212\u001b[0m, in \u001b[0;36mDDPMScheduler.p_sample\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    210\u001b[0m b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    211\u001b[0m batched_times \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), t, device \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdevice, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong) \u001b[39m# vector of length dim0(x) filled with t\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m model_mean, _, model_log_variance, x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(x \u001b[39m=\u001b[39;49m x, t \u001b[39m=\u001b[39;49m batched_times, obj_cond \u001b[39m=\u001b[39;49m obj_cond, edge_cond \u001b[39m=\u001b[39;49m edge_cond, relation_cond \u001b[39m=\u001b[39;49m relation_cond, cond_scale \u001b[39m=\u001b[39;49m cond_scale, clip_denoised \u001b[39m=\u001b[39;49m clip_denoised)\n\u001b[1;32m    213\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x) \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0.\u001b[39m \u001b[39m# no noise if t == 0\u001b[39;00m\n\u001b[1;32m    214\u001b[0m pred_img \u001b[39m=\u001b[39m model_mean \u001b[39m+\u001b[39m (\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m model_log_variance)\u001b[39m.\u001b[39mexp() \u001b[39m*\u001b[39m noise\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:193\u001b[0m, in \u001b[0;36mDDPMScheduler.p_mean_variance\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\u001b[39mself\u001b[39m, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_denoised \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    187\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m    Given x_t, t and graph_cond (obj_cond [BxNxC], edge_cond [Bx2xE], relation_cond [BxE])\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    --> gets x_0 from 'model_predictions'\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m    --> sends x_0, x_t, t to 'y_posterior' to recover mean and variance of p(x_t-1 | x_0, x_t, t)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m    --> outputs mÃ¼, var, log_var, x_0\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_predictions(x, t, obj_cond, edge_cond, relation_cond, cond_scale) \u001b[39m# preds = ('ModelPrediction', ['pred_noise', 'pred_x_start'])\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     x_start \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mpred_x_start\n\u001b[1;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m clip_denoised:\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:169\u001b[0m, in \u001b[0;36mDDPMScheduler.model_predictions\u001b[0;34m(self, x, t, obj_cond, edge_cond, relation_cond, cond_scale, clip_x_start)\u001b[0m\n\u001b[1;32m    167\u001b[0m     x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_start_from_noise(x, t, pred_noise)\n\u001b[1;32m    168\u001b[0m     \u001b[39m#x_start = maybe_clip(x_start)\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     x_start \u001b[39m=\u001b[39m DDPMUtils\u001b[39m.\u001b[39;49mdynamic_clip(x_start)\n\u001b[1;32m    171\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpred_x0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    172\u001b[0m     x_start \u001b[39m=\u001b[39m model_output\n",
      "File \u001b[0;32m~/Projects/Lectures/ADL4CV/Guided-3D-Scene-Synthesis-using-DDPMs/guided-diffusion/ddpm_scheduler.py:413\u001b[0m, in \u001b[0;36mDDPMUtils.dynamic_clip\u001b[0;34m(A, p)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    411\u001b[0m \u001b[39m# TODO: what should be the default value of p?\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdynamic_clip\u001b[39m(A, p\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m): \n\u001b[0;32m--> 413\u001b[0m     s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mquantile(torch\u001b[39m.\u001b[39;49mflatten(torch\u001b[39m.\u001b[39;49mabsolute(A), start_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), p, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    414\u001b[0m     s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(s, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    415\u001b[0m     expanded_s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mview(A\u001b[39m.\u001b[39msize(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(A\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: quantile() input tensor must be non-empty"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader:\n",
    "    x_batch = batch.x.to(device)[:, C:]\n",
    "    obj_cond_batch = batch.cond.to(device)\n",
    "    edge_cond_batch = batch.edge_index.to(device)\n",
    "    relation_cond_batch = batch.edge_attr.to(device)\n",
    "    \n",
    "    # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "    x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "    # obj_cond is read as [B*N, C] and needs to be reshaped to [B, N, C]\n",
    "    obj_cond_batch = obj_cond_batch.view(batch.num_graphs, N, C)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():      \n",
    "        # Sample from the model (use the same conditioning as the overfitting)\n",
    "        sampled_scene = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=5.0, return_all_samples=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_semseg_file(sampled_scene, scan_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    x_batch = batch.x.to(device)[:, C:]\n",
    "    obj_cond_batch = batch.cond.to(device)\n",
    "    edge_cond_batch = batch.edge_index.to(device)\n",
    "    relation_cond_batch = batch.edge_attr.to(device)\n",
    "    \n",
    "    # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "    x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "    # obj_cond is read as [B*N, C] and needs to be reshaped to [B, N, C]\n",
    "    obj_cond_batch = obj_cond_batch.view(batch.num_graphs, N, C)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():      \n",
    "        # Sample from the model (use the same conditioning as the overfitting)\n",
    "        # (!) NOTICE: this will return all the samples from the scheduler\n",
    "        sampled_scenes = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=5.0, return_all_samples=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map all the sampled scenes to semseg files\n",
    "for t, sampled_scene in sampled_scenes:\n",
    "    generate_semseg_file(sampled_scene, scan_id=t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DVIS Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dvis import dvis\n",
    "from mathutils import Matrix\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_rotation(normalized_axes, rotation_angle, rotation_axis):\n",
    "    # Convert rotation angle to radians\n",
    "    rotation_angle_rad = np.deg2rad(rotation_angle)\n",
    "\n",
    "    if rotation_axis == 'x':\n",
    "        rotation_matrix = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(rotation_angle_rad), -np.sin(rotation_angle_rad)],\n",
    "            [0, np.sin(rotation_angle_rad), np.cos(rotation_angle_rad)]\n",
    "        ])\n",
    "    elif rotation_axis == 'y':\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(rotation_angle_rad), 0, np.sin(rotation_angle_rad)],\n",
    "            [0, 1, 0],\n",
    "            [-np.sin(rotation_angle_rad), 0, np.cos(rotation_angle_rad)]\n",
    "        ])\n",
    "    elif rotation_axis == 'z':\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(rotation_angle_rad), -np.sin(rotation_angle_rad), 0],\n",
    "            [np.sin(rotation_angle_rad), np.cos(rotation_angle_rad), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid rotation axis. Supported values are 'x', 'y', and 'z'.\")\n",
    "    \n",
    "    encoded_normalized_axes = np.dot(normalized_axes, rotation_matrix)\n",
    "\n",
    "    return encoded_normalized_axes\n",
    "\n",
    "def translate_corners(corners, translation):\n",
    "    translated_corners = corners + translation\n",
    "    return translated_corners\n",
    "\n",
    "# Unit cube definition\n",
    "unit_cube_corners = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    \n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "centroid = np.mean(unit_cube_corners, axis=0)\n",
    "unit_cube_corners = unit_cube_corners - centroid\n",
    "\n",
    "# Original normalized_axes matrix representing the unit cube's orientation\n",
    "normalized_axes = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Encode degree rotation around the axis\n",
    "rotation_angle, axis = 0, 'y'\n",
    "encoded_normalized_axes = encode_rotation(normalized_axes, rotation_angle, axis)\n",
    "\n",
    "# Apply the encoded rotation to the unit cube corners\n",
    "unit_cube_corners = np.dot(unit_cube_corners, encoded_normalized_axes)\n",
    "\n",
    "# Translate the rotated cube\n",
    "translation = np.array([0, 0, 0])\n",
    "unit_cube_corners = translate_corners(unit_cube_corners, translation)\n",
    "\n",
    "# dvis(unit_cube_corners, 'corners', c=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the dataset folder\n",
    "dataset_path = 'datasets/data'\n",
    "\n",
    "def generate_corners(obj):\n",
    "    obb = obj['obb']\n",
    "    axes_lengths = obb['axesLengths']\n",
    "    centroid = obb['centroid']\n",
    "    normalized_axes = np.reshape(obb['normalizedAxes'], (3, 3))\n",
    "    \n",
    "    axes_lengths = np.array(axes_lengths)\n",
    "    centroid = np.array(centroid)\n",
    "    normalized_axes = np.array(normalized_axes)\n",
    "    \n",
    "    # Swap y and z axes\n",
    "    # normalized_axes[[1, 2]] = normalized_axes[[2, 1]] # TODO: rotation is off\n",
    "    axes_lengths[[1, 2]] = axes_lengths[[2, 1]]\n",
    "    centroid[[1, 2]] = centroid[[2, 1]]\n",
    "\n",
    "    corners = np.zeros((8, 3))\n",
    "    for i in range(8):\n",
    "        corner = unit_cube_corners[i]\n",
    "        scaled_corner = corner * axes_lengths\n",
    "        transformed_corner = np.dot(normalized_axes, scaled_corner)\n",
    "        corners[i] = transformed_corner + centroid\n",
    "\n",
    "    return corners\n",
    "\n",
    "def visualize_scene(scene_id, t=0):\n",
    "    scan_folder_path = os.path.join(dataset_path, scene_id)\n",
    "\n",
    "    # Check if the folder contains semseg.v2.json file\n",
    "    semseg_file = os.path.join(scan_folder_path, f'{t}_semseg.v2.json')\n",
    "    if not os.path.isfile(semseg_file):\n",
    "        exit(1)\n",
    "\n",
    "    # Read and parse the semseg.v2.json file\n",
    "    with open(semseg_file, 'r') as file:\n",
    "        semseg_data = json.load(file)\n",
    "\n",
    "    scan_id = semseg_data['scan_id']\n",
    "    seg_groups = semseg_data['segGroups']\n",
    "    \n",
    "    colors_labels_map = {}\n",
    "    col_index = 0\n",
    "\n",
    "    for obj in seg_groups:\n",
    "        # if obj['dominantNormal'][0] != 0:\n",
    "        #     continue\n",
    "        \n",
    "        # if obj['label'] in ['wall', 'floor', 'ceiling']:\n",
    "        #     continue\n",
    "        \n",
    "        corners = generate_corners(obj)\n",
    "        \n",
    "        # print(obj['label'])\n",
    "        \n",
    "        colors_labels_map[obj['label']] = colors_labels_map.get(obj['label'], col_index)\n",
    "        col_index += 1\n",
    "        # Pass the corners to the visualizer\n",
    "        dvis(corners, \"corners\", name=obj['label'], c=colors_labels_map[obj['label']], t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = 'gen'\n",
    "\n",
    "# Single scene visualization\n",
    "# visualize_scene(scene_id)\n",
    "\n",
    "# Visualize certain timesteps in the reverse order (clean to noisy)\n",
    "for t in range(0, 500, 20):\n",
    "    visualize_scene(scene_id, t=t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
