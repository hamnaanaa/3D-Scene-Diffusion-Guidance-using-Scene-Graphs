{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler\n",
    "from scenes_dataset import ScenesDataset, DatasetConstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']\n",
    "  \n",
    "# Not available yet  \n",
    "# with open('datasets/data/test.json', 'r') as file:\n",
    "#     test_data = json.load(file)['scenes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 128 # num of scenes in batch\n",
    "\n",
    "# Scene hyperparams\n",
    "N = 20 # num of objects in scene\n",
    "D = 15 # dim of objects from the scene\n",
    "\n",
    "# Time hyperparams\n",
    "T = 14\n",
    "\n",
    "# Condition hyperparmas\n",
    "C = 300 # dim of node features\n",
    "R = 23+1 # num of relations\n",
    "\n",
    "hparams = {\n",
    "    # constants\n",
    "    'epochs': 2000, 'scheduler_loss': 'l2', 'rgc_activation': 'tanh',\n",
    "    # from hparam search\n",
    "    'batch_size': 32, 'time_dim': 44, 'rgc_hidden_dims': '()', 'rgc_num_bases': 4, 'rgc_aggr': 'mean', 'rgc_dp_rate': 0.14463856683812687, 'rgc_bias': False, 'attention_self_head_dims': 30, 'attention_num_heads': 1, 'attention_cross_head_dims': 30, 'scheduler_timesteps': 1000, 'scheduler_beta_schedule': 'linear', 'cfg_cond_drop_prob': 0.16303181894889107, 'optimizer_lr': 0.000571096217369203, 'optimizer_weight_decay': 0.00010261093147577781, 'lr_scheduler_factor': 0.813888153675873, 'lr_scheduler_patience': 60, 'lr_scheduler_minlr': 0.00036368282361166394\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_params = {\n",
    "    \"num_obj\": N,\n",
    "    \"obj_cond_dim\": C,\n",
    "    'layer_1_dim': D,\n",
    "    'layer_2_dim': D + hparams['time_dim'],\n",
    "    \"time_dim\": hparams['time_dim'],\n",
    "}\n",
    "\n",
    "attention_params = {\n",
    "    \"attention_self_head_dim\": hparams['attention_self_head_dims'],\n",
    "    \"attention_num_heads\": hparams['attention_num_heads'],\n",
    "    \"attention_cross_head_dim\": hparams['attention_cross_head_dims']\n",
    "}\n",
    "\n",
    "rgc_params = {\n",
    "    \"rgc_hidden_dims\": hparams['rgc_hidden_dims'],\n",
    "    \"rgc_num_relations\": R,\n",
    "    \"rgc_num_bases\": hparams['rgc_num_bases'],\n",
    "    \"rgc_aggr\": hparams['rgc_aggr'],\n",
    "    \"rgc_activation\": hparams['rgc_activation'],\n",
    "    \"rgc_dp_rate\": hparams['rgc_dp_rate'],\n",
    "    \"rgc_bias\": hparams['rgc_bias']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# --- Load the data\n",
    "range_matrix = DatasetConstants.get_range_matrix().to(device)\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    general_params=general_params,\n",
    "    attention_params=attention_params,\n",
    "    rgc_params=rgc_params,\n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "# load the best model\n",
    "model.load_state_dict(torch.load('models/val-model_0146_l2_all+CFG.pt'))\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    range_matrix = range_matrix,\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "model.eval()\n",
    "scheduler.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_dataset = ScenesDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "val_dataset = ScenesDataset(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semseg_file(sampled_scene, labels=None, scan_id=0):\n",
    "    # Take the first sample in the batch\n",
    "    filtered_scene = sampled_scene\n",
    "    \n",
    "    # Take the first sample in the batch\n",
    "    labels = labels\n",
    "\n",
    "    objs = []\n",
    "    for i in range(20):\n",
    "        label = labels[i]\n",
    "        location = filtered_scene[i, 0:3]\n",
    "        normalized_axes = filtered_scene[i, 3:12]\n",
    "        sizes = filtered_scene[i, 12:15]\n",
    "        \n",
    "        objs.append({\n",
    "            'obb': {\n",
    "                'centroid': location.tolist(),\n",
    "                'normalizedAxes': normalized_axes.tolist(),\n",
    "                'axesLengths': sizes.tolist()\n",
    "            },\n",
    "            'label': label,\n",
    "            'dominantNormal': [0, 0, 0], # not used for now\n",
    "        })\n",
    "\n",
    "    # Store the sampled scene to visualize using DVIS\n",
    "    encoded_scene = {\n",
    "        'scan_id': scan_id,\n",
    "        'segGroups': objs, # TODO: add segGroups\n",
    "    }\n",
    "\n",
    "    # save the sampled scene to a JSON file (create the folder if it doesn't exist)\n",
    "    with open(f'datasets/data/gen/{scan_id}_semseg.v2.json', 'w') as file:\n",
    "        json.dump(encoded_scene, file, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DVIS Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dvis import dvis\n",
    "from mathutils import Matrix\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_rotation(normalized_axes, rotation_angle, rotation_axis):\n",
    "    # Convert rotation angle to radians\n",
    "    rotation_angle_rad = np.deg2rad(rotation_angle)\n",
    "\n",
    "    if rotation_axis == 'x':\n",
    "        rotation_matrix = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(rotation_angle_rad), -np.sin(rotation_angle_rad)],\n",
    "            [0, np.sin(rotation_angle_rad), np.cos(rotation_angle_rad)]\n",
    "        ])\n",
    "    elif rotation_axis == 'y':\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(rotation_angle_rad), 0, np.sin(rotation_angle_rad)],\n",
    "            [0, 1, 0],\n",
    "            [-np.sin(rotation_angle_rad), 0, np.cos(rotation_angle_rad)]\n",
    "        ])\n",
    "    elif rotation_axis == 'z':\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(rotation_angle_rad), -np.sin(rotation_angle_rad), 0],\n",
    "            [np.sin(rotation_angle_rad), np.cos(rotation_angle_rad), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid rotation axis. Supported values are 'x', 'y', and 'z'.\")\n",
    "    \n",
    "    encoded_normalized_axes = np.dot(normalized_axes, rotation_matrix)\n",
    "\n",
    "    return encoded_normalized_axes\n",
    "\n",
    "def translate_corners(corners, translation):\n",
    "    translated_corners = corners + translation\n",
    "    return translated_corners\n",
    "\n",
    "# Unit cube definition\n",
    "unit_cube_corners = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    \n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "centroid = np.mean(unit_cube_corners, axis=0)\n",
    "unit_cube_corners = unit_cube_corners - centroid\n",
    "\n",
    "# Original normalized_axes matrix representing the unit cube's orientation\n",
    "normalized_axes = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Encode degree rotation around the axis\n",
    "rotation_angle, axis = 0, 'y'\n",
    "encoded_normalized_axes = encode_rotation(normalized_axes, rotation_angle, axis)\n",
    "\n",
    "# Apply the encoded rotation to the unit cube corners\n",
    "unit_cube_corners = np.dot(unit_cube_corners, encoded_normalized_axes)\n",
    "\n",
    "# Translate the rotated cube\n",
    "translation = np.array([0, 0, 0])\n",
    "unit_cube_corners = translate_corners(unit_cube_corners, translation)\n",
    "\n",
    "# dvis(unit_cube_corners, 'corners', c=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the dataset folder\n",
    "dataset_path = 'datasets/data'\n",
    "\n",
    "def generate_corners(obj):\n",
    "    obb = obj['obb']\n",
    "    axes_lengths = obb['axesLengths']\n",
    "    centroid = obb['centroid']\n",
    "    normalized_axes = np.reshape(obb['normalizedAxes'], (3, 3))\n",
    "    \n",
    "    axes_lengths = np.array(axes_lengths)\n",
    "    centroid = np.array(centroid)\n",
    "    normalized_axes = np.array(normalized_axes)\n",
    "    \n",
    "    # Swap y and z axes\n",
    "    # normalized_axes[[1, 2]] = normalized_axes[[2, 1]] # TODO: rotation is off\n",
    "    axes_lengths[[1, 2]] = axes_lengths[[2, 1]]\n",
    "    centroid[[1, 2]] = centroid[[2, 1]]\n",
    "\n",
    "    corners = np.zeros((8, 3))\n",
    "    for i in range(8):\n",
    "        corner = unit_cube_corners[i]\n",
    "        scaled_corner = corner * axes_lengths\n",
    "        transformed_corner = np.dot(normalized_axes, scaled_corner)\n",
    "        corners[i] = transformed_corner + centroid\n",
    "\n",
    "    return corners\n",
    "\n",
    "\n",
    "def visualize_gt_dataset(split='val', t_max=44, T=None):\n",
    "    \"\"\"Visualize the ground truth scenes after filtering (same json file as the one used for training/val)\"\"\"\n",
    "    semseg_file = os.path.join(dataset_path, f'{split}.json')\n",
    "    \n",
    "    if not os.path.isfile(semseg_file):\n",
    "        exit(1)\n",
    "        \n",
    "    with open(semseg_file, 'r') as file:\n",
    "        semseg_data = json.load(file)['scenes']\n",
    "        \n",
    "    for t, scene in enumerate(semseg_data):\n",
    "        if t > t_max:\n",
    "            break\n",
    "        \n",
    "        if T is not None and t != T:\n",
    "            continue\n",
    "        \n",
    "        scan_id = scene['scene_id']\n",
    "        scene_matrix = scene['scene_matrix']\n",
    "        labels = scene['labels']\n",
    "        \n",
    "        colors_labels_map = {}\n",
    "        col_index = 0\n",
    "\n",
    "        for i, row in enumerate(scene_matrix):\n",
    "            if labels[i] == 'none':\n",
    "                continue\n",
    "            \n",
    "            obj = {\n",
    "                'obb': {\n",
    "                    'centroid': row[0:3],\n",
    "                    'normalizedAxes': row[3:12],\n",
    "                    'axesLengths': row[12:15]\n",
    "                }\n",
    "            }            \n",
    "            corners = generate_corners(obj)\n",
    "            \n",
    "            \n",
    "            colors_labels_map[labels[i]] = colors_labels_map.get(labels[i], col_index)\n",
    "            col_index += 1\n",
    "            # Pass the corners to the visualizer\n",
    "            dvis(corners, \"corners\", name=labels[i], c=colors_labels_map[labels[i]], t=t)\n",
    "        \n",
    "\n",
    "\n",
    "def visualize_scene(scene_id, t=0):\n",
    "    \"\"\"Visualize a single generated scene from a semseg file\"\"\"\n",
    "    scan_folder_path = os.path.join(dataset_path, scene_id)\n",
    "\n",
    "    # Check if the folder contains semseg.v2.json file\n",
    "    semseg_file = os.path.join(scan_folder_path, f'{t}_semseg.v2.json')\n",
    "    if not os.path.isfile(semseg_file):\n",
    "        exit(1)\n",
    "\n",
    "    # Read and parse the semseg.v2.json file\n",
    "    with open(semseg_file, 'r') as file:\n",
    "        semseg_data = json.load(file)\n",
    "\n",
    "    scan_id = semseg_data['scan_id']\n",
    "    seg_groups = semseg_data['segGroups']\n",
    "    \n",
    "    colors_labels_map = {}\n",
    "    col_index = 0\n",
    "\n",
    "    for i, obj in enumerate(seg_groups):\n",
    "        if obj['label'] == 'none':\n",
    "            continue\n",
    "        \n",
    "        corners = generate_corners(obj)\n",
    "        \n",
    "        # print(obj['label'])\n",
    "        \n",
    "        colors_labels_map[obj['label']] = colors_labels_map.get(obj['label'], col_index)\n",
    "        col_index += 1\n",
    "        # Pass the corners to the visualizer\n",
    "        dvis(corners, \"corners\", name=obj['label'], c=colors_labels_map[obj['label']], t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gt_dataset('val', t_max=45, T=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpm_scheduler import DDPMUtils\n",
    "\n",
    "scene_id = 44\n",
    "\n",
    "semseg_file = os.path.join(dataset_path, f'val.json')\n",
    "\n",
    "if not os.path.isfile(semseg_file):\n",
    "    exit(1)\n",
    "    \n",
    "with open(semseg_file, 'r') as file:\n",
    "    semseg_data = json.load(file)['scenes']\n",
    "    \n",
    "scene = semseg_data[scene_id]\n",
    "\n",
    "scene_matrix = scene['scene_matrix']\n",
    "labels = scene['labels']\n",
    "\n",
    "colors_labels_map = {}\n",
    "col_index = 0\n",
    "\n",
    "shelf_counter = 0\n",
    "chair_counter = 0\n",
    "counter_counter = 0\n",
    "\n",
    "T = 200\n",
    "betas = DDPMUtils.linear_beta_schedule(T)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0) # alpha_hat_t for every timestep\n",
    "        \n",
    "\n",
    "for i, row in enumerate(scene_matrix):\n",
    "    if labels[i] == 'shelf':\n",
    "        shelf_counter += 1\n",
    "    if labels[i] == 'chair':\n",
    "        chair_counter += 1\n",
    "    if labels[i] == 'counter':\n",
    "        counter_counter += 1\n",
    "    if labels[i] in ['none', 'sink'] or (labels[i] == 'shelf' and shelf_counter > 1) or (labels[i] == 'chair' and chair_counter > 2):\n",
    "        continue\n",
    "    \n",
    "    for t in range(50):\n",
    "        row = torch.tensor(row)\n",
    "        # apply diffusion to the row depending on the time step\n",
    "        row = torch.sqrt(alphas_cumprod[t]) * row + torch.sqrt(1. - alphas_cumprod[t]) * torch.randn_like(row)\n",
    "        \n",
    "        obj = {\n",
    "            'obb': {\n",
    "                'centroid': row[0:3],\n",
    "                'normalizedAxes': row[3:12],\n",
    "                'axesLengths': row[12:15]\n",
    "            }\n",
    "        }            \n",
    "        corners = generate_corners(obj)\n",
    "        \n",
    "        if t == 0:\n",
    "            colors_labels_map[labels[i]] = colors_labels_map.get(labels[i], col_index)\n",
    "            col_index += 1\n",
    "            \n",
    "        # Pass the corners to the visualizer\n",
    "        dvis(corners, \"corners\", name=labels[i], c=colors_labels_map[labels[i]], t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion process inference\n",
    "\n",
    "Visualize how a scene gets diffused over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    # x_batch = batch.x.to(device)\n",
    "    obj_cond_batch = batch.cond.to(device)\n",
    "    edge_cond_batch = batch.edge_index.to(device)\n",
    "    relation_cond_batch = batch.edge_attr.to(device)\n",
    "    \n",
    "    # obj_cond is read as [B*N, C] and needs to be reshaped to [B, N, C]\n",
    "    obj_cond_batch = obj_cond_batch.view(batch.num_graphs, N, C)\n",
    "    \n",
    "    labels_batch = batch.labels\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():      \n",
    "        # Sample from the model (use the same conditioning as the overfitting)\n",
    "        # (!) NOTICE: this will return all the samples from the scheduler\n",
    "        sampled_scenes = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=3., return_all_samples=True)    \n",
    "        # use only first sample in the batch\n",
    "        for scene_pair in sampled_scenes:\n",
    "            t, t_sampled_scenes = scene_pair\n",
    "            # only visualize one scene from the batch\n",
    "            sampled_scene = t_sampled_scenes[0]\n",
    "            labels = labels_batch[0]\n",
    "            generate_semseg_file(sampled_scene, labels=labels, scan_id=t)\n",
    "\n",
    "    # do one batch only\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4, 12\n",
    "\n",
    "i=17\n",
    "\n",
    "for scene_pair in sampled_scenes:\n",
    "    t, t_sampled_scenes = scene_pair\n",
    "    # only visualize one scene from the batch\n",
    "    sampled_scene = t_sampled_scenes[i]\n",
    "    labels = labels_batch[i]\n",
    "    generate_semseg_file(sampled_scene, labels=labels, scan_id=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = 'gen'\n",
    "\n",
    "# Single scene visualization\n",
    "# visualize_scene(scene_id)\n",
    "\n",
    "# Visualize certain timesteps in the reverse order (clean to noisy)\n",
    "for t in range(0, 400, 10):\n",
    "    visualize_scene(scene_id, t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking alignment\n",
    "\n",
    "Generate scenes from all conditions from val_dataset, visualize them, and manually compute alignment score (see paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    # x_batch = batch.x.to(device)\n",
    "    obj_cond_batch = batch.cond.to(device)\n",
    "    edge_cond_batch = batch.edge_index.to(device)\n",
    "    relation_cond_batch = batch.edge_attr.to(device)\n",
    "    \n",
    "    # X is read as [B*N, D] and needs to be reshaped to [B, N, D]\n",
    "    # x_batch = x_batch.view(batch.num_graphs, N, D)\n",
    "    # obj_cond is read as [B*N, C] and needs to be reshaped to [B, N, C]\n",
    "    obj_cond_batch = obj_cond_batch.view(batch.num_graphs, N, C)\n",
    "    \n",
    "    labels_batch = batch.labels\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():      \n",
    "        # Sample from the model (use the same conditioning as the overfitting)\n",
    "        # (!) NOTICE: this will return all the samples from the scheduler\n",
    "        sampled_scenes = scheduler.sample(obj_cond_batch, edge_cond_batch, relation_cond_batch, cond_scale=3., return_all_samples=False)    \n",
    "        for i in range(sampled_scenes.shape[0]):\n",
    "            generate_semseg_file(sampled_scenes[i], labels=labels_batch[i], scan_id=counter)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = 'val'\n",
    "\n",
    "# Visualize certain timesteps in the reverse order (clean to noisy)\n",
    "# Scan all folders inside the dataset/val folder\n",
    "for t, folder in enumerate(os.listdir(os.path.join(dataset_path, val_path))):\n",
    "    if t == 0:\n",
    "        visualize_scene(os.path.join(val_path, folder), t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "all_relationships = ['left', 'right', 'close by', 'behind', 'front', 'attached to', 'standing on', 'lower than', 'higher than', 'lying on', 'smaller than', 'bigger than', 'hanging on', 'supported by', 'standing in', 'leaning against', 'build in', 'lying in', 'connected to', 'belonging to', 'cover', 'part of', 'hanging in']\n",
    "\n",
    "# Get the t-th scene from (!) val_data and extract labels and rel_cond from it\n",
    "scene = val_data[t]\n",
    "\n",
    "# Get the labels from the scene\n",
    "labels = scene['labels']\n",
    "edges = scene['graph_edges']\n",
    "relationships = scene['graph_relationships']\n",
    "\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Edges: {edges}\")\n",
    "print(f\"Relationships: {relationships}\")\n",
    "\n",
    "# For every edge, get the relationship from the relationships list as well as the two objects from the labels list\n",
    "# Then, generate a human-readable description of the relationship between the two objects\n",
    "for i in range(len(edges[0])):\n",
    "    # Get the two objects from the labels list\n",
    "    obj1 = labels[edges[0][i]]\n",
    "    obj2 = labels[edges[1][i]]\n",
    "\n",
    "    # Get the relationship from the relationships list\n",
    "    relationship = all_relationships[relationships[i]]\n",
    "\n",
    "    # Generate a human-readable description of the relationship between the two objects\n",
    "    print(f\"{obj1} is {relationship} {obj2}\")\n",
    "\n",
    "print(f\"Total number of relationships: {len(relationships)} for scene id: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gt_results = {\n",
    "    # example\n",
    "    # '-1': ([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1], 12)\n",
    "    '0': ([], )\n",
    "}\n",
    "\n",
    "def compute_alignment(results):\n",
    "    # iterate over manual_results and compute the avg. number of correct predictions\n",
    "    avg_alignment = 0\n",
    "    for key, value in results.items():\n",
    "        # get the predicted relationships for the scene\n",
    "        predicted_relationships = value[0]\n",
    "        # get the number of correct predictions\n",
    "        num_relations = value[1]\n",
    "        # compute the avg. number of correct predictions\n",
    "        avg_num_correct_predictions = np.sum(predicted_relationships) / num_relations\n",
    "        avg_alignment += avg_num_correct_predictions\n",
    "\n",
    "    avg_alignment /= len(results)\n",
    "    return avg_alignment\n",
    "    \n",
    "print(compute_alignment(gt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
