{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import from guided-diffusion folder\n",
    "from model import GuidedDiffusionNetwork\n",
    "from ddpm_scheduler import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, scenes):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.scenes = scenes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene = self.scenes[index]\n",
    "        \n",
    "        scene_matrix = torch.tensor(scene[\"scene_matrix\"], dtype=torch.float32)\n",
    "        graph_objects = torch.tensor(scene[\"graph_objects\"], dtype=torch.float32)\n",
    "        graph_edges = torch.tensor(scene[\"graph_edges\"], dtype=torch.long)\n",
    "        graph_relationships = torch.tensor(scene[\"graph_relationships\"], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'x': scene_matrix,\n",
    "            'obj_cond': graph_objects,\n",
    "            'edge_cond': graph_edges,\n",
    "            'relation_cond': graph_relationships\n",
    "        }\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x_batch = torch.stack([item['x'] for item in batch], dim=0)\n",
    "        obj_cond_batch = torch.cat([item['obj_cond'] for item in batch], dim=0)\n",
    "        edge_cond_batch = torch.cat([item['edge_cond'] for item in batch], dim=1)\n",
    "        relation_cond_batch = torch.cat([item['relation_cond'] for item in batch], dim=0)\n",
    "\n",
    "        return {\n",
    "            'x': x_batch,\n",
    "            'obj_cond': obj_cond_batch,\n",
    "            'edge_cond': edge_cond_batch,\n",
    "            'relation_cond': relation_cond_batch\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('datasets/data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)['scenes']\n",
    "\n",
    "with open('datasets/data/val.json', 'r') as file:\n",
    "    val_data = json.load(file)['scenes']\n",
    "  \n",
    "# Not available yet  \n",
    "# with open('datasets/data/test.json', 'r') as file:\n",
    "#     test_data = json.load(file)['scenes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range matrix for real data\n",
    "text_max = torch.ones(300)\n",
    "text_min = -torch.ones(300)\n",
    "\n",
    "location_max = torch.tensor([3.285, 3.93, 0.879])\n",
    "location_min = torch.tensor([-3.334, -2.619, -1.329])\n",
    "\n",
    "normalized_axes_max = torch.ones(9)\n",
    "normalized_axes_min = -torch.ones(9)\n",
    "\n",
    "size_max = torch.tensor([4.878, 2.655, 2.305])\n",
    "size_min = torch.tensor([0.232, 0.14, 0.094])\n",
    "\n",
    "range_max = torch.cat((text_max, location_max, normalized_axes_max, size_max), dim=0)\n",
    "range_min = torch.cat((text_min, location_min, normalized_axes_min, size_min), dim=0)\n",
    "\n",
    "range_matrix = torch.cat((range_max.unsqueeze(0), range_min.unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64 # num of scenes in batch\n",
    "\n",
    "# Scene hyperparams\n",
    "N = 20 # num of nodes\n",
    "\n",
    "# RGCN hyperparams\n",
    "C = 300 # dim of node features\n",
    "R = 23 # num of relations\n",
    "\n",
    "# Attention hyperparams\n",
    "D = 315 # dim of objects from the scene\n",
    "\n",
    "hparams = {\n",
    "    'batch_size': B, # num of graphs in batch\n",
    "    \n",
    "    # --- Attention hyperparams ---\n",
    "    'attention_out_dim': D,\n",
    "    'attention_num_heads': 5, # must be a divisor of D\n",
    "    \n",
    "    # --- Encoder RGCN hyperparams ---\n",
    "    'encoder_out_dim': C,\n",
    "    'encoder_hidden_dims': f\"{()}\", # (C, C),\n",
    "    'encoder_num_bases': None,\n",
    "    'encoder_aggr': 'mean',\n",
    "    'encoder_activation': 'leakyrelu',\n",
    "    'encoder_dp_rate': 0.,\n",
    "    'encoder_bias': True,\n",
    "    \n",
    "    # --- Fusion RGCN hyperparams ---\n",
    "    'fusion_hidden_dims': f\"{()}\", # (C+D, C+D, D),\n",
    "    'fusion_num_bases': None,\n",
    "    'fusion_aggr': 'mean',\n",
    "    'fusion_activation': 'leakyrelu',\n",
    "    'fusion_dp_rate': 0.,\n",
    "    'fusion_bias': True,\n",
    "    \n",
    "    # Scheduler hyperparams\n",
    "    'scheduler_timesteps': 1000,\n",
    "    'scheduler_loss': 'l1',\n",
    "    'scheduler_beta_schedule': 'cosine',\n",
    "    # Note: not needed for now\n",
    "    # 'scheduler_sampling_timesteps': None,\n",
    "    # \"scheduler_objective\": 'pred_noise',\n",
    "    # 'scheduler_ddim_sampling_eta': 1.0,\n",
    "    # 'scheduler_min_snr_loss_weight': False,\n",
    "    # 'scheduler_min_snr_gamma': 5,\n",
    "    \n",
    "    # Classifier-free guidance parameters\n",
    "    'cfg_cond_drop_prob': 0.1,\n",
    "    \n",
    "    # Training and optimizer hyperparams\n",
    "    'epochs': 5000,\n",
    "    'optimizer_lr': 1e-3,\n",
    "    'optimizer_weight_decay': 5e-5,\n",
    "    'lr_scheduler_factor': 0.8,\n",
    "    'lr_scheduler_patience': 20,\n",
    "    'lr_scheduler_minlr': 8e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# Not all operations support MPS yet so this option is not available for now\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "range_matrix = range_matrix.to(device)\n",
    "\n",
    "# --- Instantiate the model\n",
    "model = GuidedDiffusionNetwork(\n",
    "    attention_in_dim=D,\n",
    "    attention_out_dim=hparams['attention_out_dim'],\n",
    "    attention_num_heads=hparams['attention_num_heads'],\n",
    "    \n",
    "    rgcn_num_relations=R,\n",
    "    \n",
    "    encoder_in_dim=C,\n",
    "    encoder_out_dim=hparams['encoder_out_dim'],\n",
    "    encoder_hidden_dims=hparams['encoder_hidden_dims'],\n",
    "    encoder_num_bases=hparams['encoder_num_bases'],\n",
    "    encoder_aggr=hparams['encoder_aggr'],\n",
    "    encoder_activation=hparams['encoder_activation'],\n",
    "    encoder_dp_rate=hparams['encoder_dp_rate'],\n",
    "    encoder_bias=hparams['encoder_bias'],\n",
    "    \n",
    "    fusion_hidden_dims=hparams['fusion_hidden_dims'],\n",
    "    fusion_num_bases=hparams['fusion_num_bases'],\n",
    "    fusion_aggr=hparams['fusion_aggr'],\n",
    "    fusion_activation=hparams['fusion_activation'],\n",
    "    fusion_dp_rate=hparams['fusion_dp_rate'],\n",
    "    fusion_bias=hparams['fusion_bias'],\n",
    "    \n",
    "    cond_drop_prob=hparams['cfg_cond_drop_prob']\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/best-model_035.pt'))\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    model=model,\n",
    "    N=N,\n",
    "    D=D,\n",
    "    range_matrix = range_matrix,\n",
    "    timesteps=hparams['scheduler_timesteps'],\n",
    "    sampling_timesteps=None,\n",
    "    loss_type=hparams['scheduler_loss'],\n",
    "    objective='pred_noise',\n",
    "    beta_schedule=hparams['scheduler_beta_schedule'],\n",
    "    ddim_sampling_eta=1.0,\n",
    "    min_snr_loss_weight=False,\n",
    "    min_snr_gamma=5\n",
    ")\n",
    "\n",
    "print(f\"DDPM Scheduler:\\n{scheduler}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "scheduler = scheduler.to(device)\n",
    "\n",
    "model.eval()\n",
    "scheduler.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference using scene conditions from the validation set\n",
    "train_dataset = CustomDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "val_dataset = CustomDataset(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    x = batch['x'].to(device)\n",
    "    obj_cond = batch['obj_cond'].to(device)\n",
    "    edge_cond = batch['edge_cond'].to(device)\n",
    "    relation_cond = batch['relation_cond'].to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():        \n",
    "        # Sample from the model (use the same conditioning as the overfitting)\n",
    "        sampled_scene = scheduler.sample(obj_cond, edge_cond, relation_cond, cond_scale=5.0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = 0\n",
    "\n",
    "# Take the first element in the batch and only the last 15 dimensions of it\n",
    "filtered_scene = sampled_scene[0, :, -15:]\n",
    "\n",
    "objs = []\n",
    "for i in range(20):\n",
    "    label = 'unknown' # TODO: generate label from neighborhood search from the embeddings\n",
    "    location = filtered_scene[i, 0:3]\n",
    "    normalized_axes = filtered_scene[i, 3:12]\n",
    "    sizes = filtered_scene[i, 12:15]\n",
    "    \n",
    "    objs.append({\n",
    "        'obb': {\n",
    "            'centroid': location.tolist(),\n",
    "            'normalizedAxes': normalized_axes.tolist(),\n",
    "            'axesLengths': sizes.tolist()\n",
    "        },\n",
    "        'label': label,\n",
    "        'dominantNormal': [0, 0, 0], # not used for now\n",
    "    })\n",
    "\n",
    "# Store the sampled scene to visualize using DVIS\n",
    "encoded_scene = {\n",
    "    'scan_id': scan_id,\n",
    "    'segGroups': objs, # TODO: add segGroups\n",
    "}\n",
    "\n",
    "# save the sampled scene to a JSON file (create the folder if it doesn't exist)\n",
    "with open(f'datasets/data/gen/scene_{scan_id}/semseg.v2.json', 'w') as file:\n",
    "    json.dump(encoded_scene, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
