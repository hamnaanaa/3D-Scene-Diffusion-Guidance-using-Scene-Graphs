{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "References\n",
    "    - https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/classifier_free_guidance.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1eccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from einops import rearrange, reduce, repeat\n",
    "#from einops.layers.torch import Rearrange\n",
    "\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ddd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# time scheduling functions\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d51f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        N,\n",
    "        D,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        loss_type = 'l1',\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'cosine',\n",
    "        ddim_sampling_eta = 1.,\n",
    "        min_snr_loss_weight = False,\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n",
    "        assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "        self.channels = self.model.channels\n",
    "\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "        \n",
    "        # Defines vector of betas based on noise scheduler\n",
    "        # --> one beta for each timestep\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0) # alpha_hat_t for every timestep\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.) # alpha_hat_(t-1): for the PREVIOUS timestep\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod)) # sqrt(alpha_hat)\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod)) # sqrt(1-alpha_hat)\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod)) # log(1-alpha_hat)\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod)) # 1/sqrt(alpha_hat)\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1)) # sqrt(1/alpha_hat -1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # beta_t_schlange\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20))) # log(beta_t)\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)) # first coefficient to calculate mü_t_schlange\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)) # second coefficient to calculate mü_t_schlange\n",
    "\n",
    "        # loss weight ???????????\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod) # signal-to-noise ratio of q(x_t | x_0)\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = maybe_clipped_snr / snr\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = maybe_clipped_snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise): # x_0 = 1/sqrt(alpha_hat)*x_t - sqrt((1-alpha_hat)/alpha_hat)eps\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0): # eps = (1/sqrt(alpha_hat)*x_t-x_0)/sqrt(1/alpha_hat-1)\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t): # q(x_t-1 | x_t, x_0)\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, classes, cond_scale = 3., clip_x_start = False):\n",
    "        model_output = self.model.forward_with_cond_scale(x, t, classes, cond_scale = cond_scale) # forward block im RGCN\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise': # would assume that we use this one?\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, classes, cond_scale, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, classes, cond_scale)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, classes, cond_scale = 3., clip_denoised = True):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((x.shape[0],), t, device = x.device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, classes = classes, cond_scale = cond_scale, clip_denoised = clip_denoised)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, classes, shape, cond_scale = 3.):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            img, x_start = self.p_sample(img, t, classes, cond_scale)\n",
    "\n",
    "        img = unnormalize_to_zero_to_one(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, classes, shape, cond_scale = 3., clip_denoised = True):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, classes, cond_scale = cond_scale, clip_x_start = clip_denoised)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        img = unnormalize_to_zero_to_one(img)\n",
    "        return img\n",
    "\n",
    "    # tbd image_size\n",
    "    @torch.no_grad()\n",
    "    def sample(self, classes, cond_scale = 3.):\n",
    "        batch_size, image_size, channels = classes.shape[0], self.image_size, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn(classes, (batch_size, channels, image_size, image_size), cond_scale)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.stack([torch.tensor(t, device = device)] * b)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def loss_fn(self):\n",
    "        if self.loss_type == 'l1':\n",
    "            return F.l1_loss\n",
    "        elif self.loss_type == 'l2':\n",
    "            return F.mse_loss\n",
    "        else:\n",
    "            raise ValueError(f'invalid loss type {self.loss_type}')\n",
    "\n",
    "    def p_losses(self, x_start, t, *, classes, noise = None):\n",
    "        b, c, h, w = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, classes)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = self.loss_fn(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "    \n",
    "    # tbd image_size\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = normalize_to_neg_one_to_one(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n",
    "\n",
    "# example\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''OUR model:\n",
    "    num_classes = 10\n",
    "\n",
    "    model = Unet(\n",
    "        dim = 64,\n",
    "        dim_mults = (1, 2, 4, 8),\n",
    "        num_classes = num_classes,\n",
    "        cond_drop_prob = 0.5\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    diffusion = GaussianDiffusion(\n",
    "        model,\n",
    "        N = 20,\n",
    "        D = 5,\n",
    "        timesteps = 1000\n",
    "    ).cuda()\n",
    "\n",
    "    training_images = torch.randn(8, 3, 128, 128).cuda() # images are normalized from 0 to 1\n",
    "    image_classes = torch.randint(0, num_classes, (8,)).cuda()    # say 10 classes\n",
    "\n",
    "    loss = diffusion(training_images, classes = image_classes)\n",
    "    loss.backward()\n",
    "\n",
    "    # do above for many steps\n",
    "\n",
    "    sampled_images = diffusion.sample(\n",
    "        classes = image_classes,\n",
    "        cond_scale = 3.                # condition scaling, anything greater than 1 strengthens the classifier free guidance. reportedly 3-8 is good empirically\n",
    "    )\n",
    "\n",
    "    sampled_images.shape # (8, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86cc3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alphas_cumprod = torch.tensor([1, 2, 3])\n",
    "a = F.pad(alphas_cumprod[:-1], (1, 0), value = 4.)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1098ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
