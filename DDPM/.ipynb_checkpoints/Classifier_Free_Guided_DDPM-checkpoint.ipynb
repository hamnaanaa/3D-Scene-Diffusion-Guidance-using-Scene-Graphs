{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43d1204",
   "metadata": {},
   "source": [
    "\"\"\"\"\n",
    "References\n",
    "    - https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/classifier_free_guidance.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42546d",
   "metadata": {},
   "source": [
    "What the model can/should do:\n",
    "<br> \n",
    "<br>\n",
    "1) Training mode <br>\n",
    "DDPM.forward <br>\n",
    "Args:\n",
    "    - batch of data X_0 [BxNxD]\n",
    "    - batch of conditional graphs [BxNxC]\n",
    "Does:\n",
    "    - samples timestep [B]\n",
    "    - samples noise eps [BxNxD]\n",
    "    - computes X_t based on q(x_t | x_0, t) = sqrt(alpha_hat)*x_0 + sqrt(1-alpha_hat)*eps #*\n",
    "    - pass X_t, t, cond_graphs to the RGCN --> predict eps_theta(x_t, t, cond_graphs)\n",
    "    - calculate loss by comparing eps and eps_theta(x_t, t, cond_graphs)\n",
    "Returns:\n",
    "    - mean loss of the batch\n",
    "<br> \n",
    "<br> \n",
    "2) Inference mode <br>\n",
    "DDPM.p_sample_loop <br>\n",
    "Args:\n",
    "    - required shape for X [BxNxD]\n",
    "    - batch of conditional graphs [BxNxC]\n",
    "    - cond_scale \n",
    "Does:\n",
    "    - samples Gaussian noise X_T [BxNxD]\n",
    "    - for every time step from T to 0\n",
    "        - send X_t, t, cond_graph to RGCN --> predict eps_theta(x_t, t, cond_graphs)\n",
    "        - computes x_0 based on x_t and eps_theta(x_t, t, cond_graphs) x_0 = 1/sqrt(alpha_hat)*x_t - sqrt((1-alpha_hat)/alpha_hat) #*\n",
    "        - computes mü_t, var_t of p(x_t-1 | x_t, x_0) given x_0, x_t, t \n",
    "        - based on mü_t, var_t of p(x_t-1 | x_t, x_0) --> predicts x_t-1\n",
    "Returns:\n",
    "    - X_0 [BxNxD]\n",
    "<br>\n",
    "<br>\n",
    "DDPM.ddim_sample <br>\n",
    "alternative, potentially more efficient --> s. paper\n",
    "<br>\n",
    "---------------------------------------------------------\n",
    "Interface management:\n",
    "<br>\n",
    "RGCN.forward_with_cond_scale <br>\n",
    "implementation s. https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/classifier_free_guidance.py row 374\n",
    "<br> \n",
    "<br>\n",
    "RGCN.forward <br>\n",
    "Args:\n",
    "    - X [BxNxD]\n",
    "    - t [B]\n",
    "    - cond_graph [BxNxC]\n",
    "    - cond_drop_prob --> probability of setting cond_graph to zero\n",
    "Returns:\n",
    "    - eps_theta(x_t, t, cond_graphs)\n",
    "<br>   \n",
    "---------------------------------------------------------\n",
    "Open questions:\n",
    "<br>\n",
    "    - normalizations (unnormalize_to_zero_to_one, normalize_to_neg_one_to_one) necessary? <br>\n",
    "    - Clipping not necessary? <br>\n",
    "    - what is loss_weight and how does it depend on snr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1eccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "#from einops.layers.torch import Rearrange\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70c61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49ddd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# time scheduling functions\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d51f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        N,\n",
    "        D,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        loss_type = 'l1',\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'cosine',\n",
    "        ddim_sampling_eta = 1.,\n",
    "        min_snr_loss_weight = False,\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #assert not (type(self) == DDPM) #and model.channels != model.out_dim)\n",
    "        #assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "        #self.channels = self.model.channels\n",
    "\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "        \n",
    "        # Defines vector of betas based on noise scheduler\n",
    "        # --> one beta for each timestep\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0) # alpha_hat_t for every timestep\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.) # alpha_hat_(t-1): for the PREVIOUS timestep\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod)) # sqrt(alpha_hat)\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod)) # sqrt(1-alpha_hat)\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod)) # log(1-alpha_hat)\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod)) # 1/sqrt(alpha_hat)\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1)) # sqrt(1/alpha_hat -1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # beta_t_schlange\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20))) # log(beta_t)\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)) # first coefficient to calculate mü_t_schlange\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)) # second coefficient to calculate mü_t_schlange\n",
    "\n",
    "        # loss weight ???????????\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod) # signal-to-noise ratio of q(x_t | x_0)\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = maybe_clipped_snr / snr\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = maybe_clipped_snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise): # x_0 = 1/sqrt(alpha_hat)*x_t - sqrt((1-alpha_hat)/alpha_hat)eps\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0): # eps = (1/sqrt(alpha_hat)*x_t-x_0)/sqrt(1/alpha_hat-1)\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t): # q(x_t-1 | x_t, x_0)\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, graph_cond, cond_scale = 3., clip_x_start = False):\n",
    "        '''\n",
    "        Given x_t [BxNxD], t [BxNxD] and graph_cond [BxNxC]\n",
    "        --> forward pass through NN\n",
    "        --> outputs eps_theta(x_t, t) and x0\n",
    "        '''\n",
    "        model_output = self.model.forward_with_cond_scale(x, t, graph_cond, cond_scale = cond_scale) # forward block im RGCN\n",
    "        #maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise': # I assume we use this one\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            #x_start = maybe_clip(x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start) # shaping output as: ('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "    def p_mean_variance(self, x, t, graph_cond, cond_scale, clip_denoised = True):\n",
    "        '''\n",
    "        Given x_t, t and graph_cond\n",
    "        --> gets x_0 from 'model_predictions'\n",
    "        --> sends x_0, x_t, t to 'y_posterior' to recover mean and variance of p(x_t-1 | x_0, x_t, t)\n",
    "        --> outputs mü, var, log_var, x_0\n",
    "        '''\n",
    "        preds = self.model_predictions(x, t, graph_cond, cond_scale) # preds = ('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, graph_cond, cond_scale = 3., clip_denoised = True):\n",
    "        '''\n",
    "        Given x_t, t and graph_cond\n",
    "        --> gets mü, log_var, x_0 from 'p_mean_variance'\n",
    "        --> predicts x_t-1\n",
    "        --> outputs x_t-1, x_0\n",
    "        '''\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((x.shape[0],), t, device = x.device, dtype = torch.long) # vector of length dim0(x) filled with t\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, graph_cond = graph_cond, cond_scale = cond_scale, clip_denoised = clip_denoised)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, graph_cond, shape, cond_scale = 3.):\n",
    "        '''\n",
    "        given an input shape: BxNxD (where B = batch size) and a graph condition\n",
    "        --> creates Gaussian noise x_T of shape BxNxD\n",
    "        --> for every timestep t from T to 0:\n",
    "            sends x_t, t, graph_cond to 'p_sample' to get x_t-1, x_0\n",
    "            sets x_t = x_t-1\n",
    "        --> outputs x_0\n",
    "        '''\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        data = torch.randn(shape, device=device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            data, x_start = self.p_sample(data, t, graph_cond, cond_scale)\n",
    "\n",
    "        # data = unnormalize_to_zero_to_one(data) # NEEDED?????\n",
    "        return data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, graph_cond, shape, cond_scale = 3., clip_denoised = True):\n",
    "        '''\n",
    "        given an input shape: BxNxD (where B = batch size)\n",
    "        --> creates Gaussian noise x_T of shape BxNxD\n",
    "        --> create time pairs [(T-1, T-2), (T-2, T-3),..., (1, 0), (0, -1)]\n",
    "        --> for every time pair (t1, t2)\n",
    "            - create a matrix time_cond of size x filled with t1\n",
    "            - send x_t, time_cond to 'model_predictions' to compute eps_theta(x_t, t) and x_0\n",
    "            - get a_hat(t1) and a_hat(t2)\n",
    "            - compute x_t-1 based on the DDIM sampler\n",
    "            - set x_t = x_t-1\n",
    "        --> output x_0\n",
    "        '''\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        data = torch.randn(shape, device = device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            pred_noise, x_start, *_ = self.model_predictions(data, time_cond, graph_cond, cond_scale = cond_scale, clip_x_start = clip_denoised)\n",
    "\n",
    "            if time_next < 0:\n",
    "                data = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(data)\n",
    "\n",
    "            data = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        #data = unnormalize_to_zero_to_one(data) # NEEDED?????\n",
    "        return data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, graph_cond, cond_scale = 3.):\n",
    "        '''\n",
    "        - gets barch size from graph_cond [BxNxC]\n",
    "        - sends the required shape BxNxD to one of the sample functions\n",
    "        - outputs x_0 [BxNxD]\n",
    "        '''\n",
    "        #batch_size, N, D, channels = classes.shape[0], self.N, self.D, self.channels\n",
    "        #sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        #return sample_fn(classes, (batch_size, channels, image_size, image_size), cond_scale)\n",
    "        batch_size, N, D = graph_cond.shape[0], self.N, self.D\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn(graph_cond, (batch_size, N, D), cond_scale)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        '''\n",
    "        given x1_0, x2_0\n",
    "        --> compute x1_t and x2_t\n",
    "        --> interpolate data_t between x1_t and x2_t\n",
    "        --> compute data_0 using 'p_sample'\n",
    "        --> output data_0\n",
    "        '''\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.stack([torch.tensor(t, device = device)] * b)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        data = (1 - lam) * xt1 + lam * xt2\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            data = self.p_sample(data, torch.full((b,), i, device=device, dtype=torch.long))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        '''\n",
    "        Given x_0, t\n",
    "        --> sample x_t\n",
    "        '''\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def loss_fn(self):\n",
    "        '''\n",
    "        return the right loss function\n",
    "        '''\n",
    "        if self.loss_type == 'l1':\n",
    "            return F.l1_loss\n",
    "        elif self.loss_type == 'l2':\n",
    "            return F.mse_loss\n",
    "        else:\n",
    "            raise ValueError(f'invalid loss type {self.loss_type}')\n",
    "\n",
    "    def p_losses(self, x_start, t, graph_cond, noise = None):\n",
    "        '''\n",
    "        Given x_0, t\n",
    "        --> sample noise\n",
    "        --> sample x_t based on noise using 'q_sample'\n",
    "        --> predicting the noise eps_theta(t) using the RGCN\n",
    "        --> calculate loss by comparing the true noise and eps_theta(t)\n",
    "        --> output mean of loss over batch\n",
    "        \n",
    "        '''\n",
    "        #b, c, h, w = x_start.shape\n",
    "        B, N, D = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model.forward(x, t, graph_cond) \n",
    "                    # müsste das nicht self.model.forward sein?????\n",
    "                    # parameter passen zu self.model.forward????\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = self.loss_fn(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "    \n",
    "    # tbd image_size\n",
    "    def forward(self, data, graph_cond, *args, **kwargs):\n",
    "        '''\n",
    "        Given Data of shape [BxNxD]\n",
    "        --> create a vector t with B random numbers between 0 and num_timesteps\n",
    "        --> returns the loss of the current NN on the batch using 'p_losses'\n",
    "        '''\n",
    "        #b, c, h, w, device, data_size, = *data.shape, data.device, self.data_size\n",
    "        #assert h == data_size and w == data_size, f'height and width of image must be {data_size}'\n",
    "        B, N, D, device = *data.shape, data.device\n",
    "        assert N == self.N and D == self.D\n",
    "        t = torch.randint(0, self.num_timesteps, (B,), device=device).long() \n",
    "\n",
    "        # data = normalize_to_neg_one_to_one(data) # NOT NEEDED?????\n",
    "        return self.p_losses(data, t, graph_cond, *args, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a32268d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61f42c166949739d57194c7686251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2465)\n",
      "tensor([[[-1.0000, -1.0000],\n",
      "         [-0.8778, -0.7627],\n",
      "         [-1.0000, -0.9926]],\n",
      "\n",
      "        [[-1.0000, -1.0000],\n",
      "         [-1.0000, -0.8398],\n",
      "         [-0.2156, -1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(\n",
    "        self):\n",
    "        super().__init__()\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        x,\n",
    "        *args,\n",
    "        cond_scale = 1.,\n",
    "        **kwargs\n",
    "        ):\n",
    "        return x\n",
    "    def forward(x, t, graph_cond):\n",
    "        return x\n",
    "    \n",
    "model = TestNet\n",
    "\n",
    "diffusion = DDPM(\n",
    "        model,\n",
    "        N=3,\n",
    "        D=2,\n",
    "        timesteps = 5,\n",
    "        sampling_timesteps = None,\n",
    "        loss_type = 'l1',\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'cosine',\n",
    "        ddim_sampling_eta = 1.,\n",
    "        min_snr_loss_weight = False,\n",
    "        min_snr_gamma = 5\n",
    "        )\n",
    "\n",
    "training_matrix = torch.rand(2, 3, 2)\n",
    "scene_graphs = torch.rand(2, 3, 1)\n",
    "\n",
    "loss = diffusion.forward(training_matrix, scene_graphs)\n",
    "\n",
    "shape = training_matrix.shape\n",
    "sample = diffusion.p_sample_loop(scene_graphs, shape, cond_scale = 3)\n",
    "\n",
    "print(loss)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1098ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example how to use\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''OUR model:\n",
    "    num_classes = 10\n",
    "\n",
    "    model = Unet(\n",
    "        dim = 64,\n",
    "        dim_mults = (1, 2, 4, 8),\n",
    "        num_classes = num_classes,\n",
    "        cond_drop_prob = 0.5\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    diffusion = DDPM(\n",
    "        model,\n",
    "        N = 20,\n",
    "        D = 5,\n",
    "        timesteps = 1000\n",
    "    ).cuda()\n",
    "\n",
    "    training_images = torch.randn(8, 3, 128, 128).cuda() # images are normalized from 0 to 1\n",
    "    image_classes = torch.randint(0, num_classes, (8,)).cuda()    # say 10 classes\n",
    "\n",
    "    loss = diffusion(training_images, classes = image_classes)\n",
    "    \n",
    "                    \n",
    "    # CLASSES WERE REPLACED BY GRAPH_COND!\n",
    "    # CHANNEL DIM WAS DELETED COMPLETELY\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # do above for many steps\n",
    "\n",
    "    sampled_images = diffusion.sample(\n",
    "        classes = image_classes,\n",
    "        cond_scale = 3.                # condition scaling, anything greater than 1 strengthens the classifier free guidance. reportedly 3-8 is good empirically\n",
    "    )\n",
    "\n",
    "    sampled_images.shape # (8, 3, 128, 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
