{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023cf1d1",
   "metadata": {},
   "source": [
    "\"\"\"\"\n",
    "References\n",
    "    - https://github.com/quickgrid/pytorch-diffusion\n",
    "    - https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/classifier_free_guidance.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c880ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "#from memory_efficient_attention_pytorch import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5c7ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device: str,\n",
    "            N: int,\n",
    "            D: int,\n",
    "            timesteps: int = 1000\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.timesteps = timesteps\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "\n",
    "        # alpha, alpha_hat, beta\n",
    "        self.beta = self.cosine_beta_schedule()\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        # sqrt(alpha_hat), sqrt(1-alpha_hat)\n",
    "        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)\n",
    "        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat)\n",
    "        \n",
    "        # sqrt(alpha), sqrt(beta)=std!\n",
    "        self.sqrt_alpha = torch.sqrt(self.alpha)\n",
    "        self.std_beta = torch.sqrt(self.beta)\n",
    "\n",
    "        # Clean up unnecessary values\n",
    "        del self.alpha\n",
    "        del self.alpha_hat\n",
    "        \n",
    "    def linear_noise_schedule(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        linear schedule, proposed in original ddpm paper\n",
    "        \"\"\"\n",
    "        scale = 1000 / self.timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, self.timesteps, dtype = torch.float64)\n",
    "        \n",
    "        \n",
    "    def cosine_beta_schedule(self):\n",
    "        \"\"\"\n",
    "        cosine schedule\n",
    "        as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "        \"\"\" \n",
    "        s = 0.008\n",
    "        steps = self.timesteps + 1\n",
    "        t = torch.linspace(0, self.timesteps, steps, dtype = torch.float64) / self.timesteps\n",
    "        alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "    def forward(self, x_0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward process (q-function):\n",
    "        --> samples x_t given x_0 and t\n",
    "        \n",
    "        Args:\n",
    "            x_0: data without noise [nxNxD]\n",
    "            t: timestep [n]\n",
    "            \n",
    "        Retruns:\n",
    "            x_t: diffused data x at timestep t [nxNxD]\n",
    "            epsilon: noise of x_t [nxNxD]\n",
    "        \"\"\"\n",
    "        sqrt_alpha_hat = self.sqrt_alpha_hat[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t].view(-1, 1, 1)\n",
    "        epsilon = torch.randn_like(x_0, device=self.device) # samples Gaussian tensor of same shape as x_0\n",
    "        return sqrt_alpha_hat * x_0 + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "    \n",
    "    def sample_timesteps(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"Timesteps selected from [1, timesteps].\n",
    "        \n",
    "        Args:\n",
    "            batch_size: int\n",
    "        \n",
    "        Returns:\n",
    "            t: randomly sampled timesteps for each sample in a batch [B]\n",
    "        \"\"\"\n",
    "        return torch.randint(low=1, high=self.timesteps, size=(batch_size,), device=self.device)\n",
    "    \n",
    "    def backward(\n",
    "            self,\n",
    "            eps_model: nn.Module,\n",
    "            n: int,\n",
    "            scale_factor: int = 2,\n",
    "            graph_cond: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Denoising Process:\n",
    "\n",
    "        Args:\n",
    "            graph_cond: tbd!!!!!\n",
    "            scale_factor: Scales the output image by the factor.\n",
    "            eps_model: Noise prediction model. `eps_theta(x_t, t)` in paper. Theta is the model parameters.\n",
    "            n: Number of samples to process.\n",
    "        \n",
    "        Returns:\n",
    "            x0: generated denoised data [nxNxD]\n",
    "        \"\"\"\n",
    "\n",
    "        #eps_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 1) sample x_T from noise (n times)\n",
    "            x = torch.randn((n, self.N, self.D), device=self.device)\n",
    "            \n",
    "            # 2) iteratively samples x_t-1 from x_t\n",
    "            # no additional noise is added when we compute x_0 from x_1!\n",
    "            for i in reversed(range(1, self.timesteps)):\n",
    "                t = torch.ones(n, dtype=torch.long, device=self.device) * i\n",
    "\n",
    "                sqrt_alpha_t = self.sqrt_alpha[t].view(-1, 1, 1)\n",
    "                beta_t = self.beta[t].view(-1, 1, 1)\n",
    "                sqrt_one_minus_alpha_hat_t = self.sqrt_one_minus_alpha_hat[t].view(-1, 1, 1)\n",
    "                epsilon_t = self.std_beta[t].view(-1, 1, 1)\n",
    "\n",
    "                random_noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)\n",
    "\n",
    "                x = ((1 / sqrt_alpha_t) *\n",
    "                     (x - ((beta_t / sqrt_one_minus_alpha_hat_t) *\n",
    "                           eps_model(\n",
    "                               x=x,\n",
    "                               t=t,\n",
    "                               graph_cond=graph_cond\n",
    "                           )))\n",
    "                     ) + (epsilon_t * random_noise)\n",
    "\n",
    "        #eps_model.train()\n",
    "\n",
    "        #x = ((x.clamp(-1, 1) + 1) * 127.5).type(torch.uint8) # Before returning values are clamped to [-1, 1] and converted to pixel values [0, 255].\n",
    "        #x = F.interpolate(input=x, scale_factor=scale_factor, mode='nearest-exact')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12a14177",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For testing\n",
    "'''\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, w):\n",
    "        self.w = w\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            t: torch.LongTensor,\n",
    "            graph_cond: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.w*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdadf8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-14.6604,  10.3582,  16.2084],\n",
      "         [  1.8696,  -5.1908,  -2.7894]],\n",
      "\n",
      "        [[ 19.9364,   3.1391,  11.0904],\n",
      "         [ -4.7591, -12.4244,  13.0996]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''Testing\n",
    "'''\n",
    "x_0 = torch.tensor([[1,2,3],[3,2,1]]).float()\n",
    "t = 3\n",
    "Diff = Diffusion(\n",
    "            device=torch.device(\"cpu\"),\n",
    "            N=2,\n",
    "            D=3,\n",
    "            timesteps= 10\n",
    "    )\n",
    "RGCN = RGCN(0.5)\n",
    "sample = Diff.forward(x_0, t)\n",
    "rand_timesteps = Diff.sample_timesteps(3)\n",
    "x_pred = Diff.backward(\n",
    "            eps_model=RGCN.forward,\n",
    "            n=2,\n",
    "            scale_factor = 2,\n",
    "            graph_cond = None,\n",
    "    )\n",
    "print(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e5f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
