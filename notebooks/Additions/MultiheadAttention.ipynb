{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38449040",
   "metadata": {},
   "source": [
    "Reference: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a116afe",
   "metadata": {},
   "source": [
    "How to use: <br>\n",
    "initialize with: <br>\n",
    "input dimension D (input_dim) <br>\n",
    "output dimension E (embed_dim) <br>\n",
    "number of attention heads (num_heads) <br>\n",
    "--> embed_dim % num_heads == 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92bceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9da7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads): # embed_dim = num_heads*D_weights! input_dim=D!\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim, bias=False) # Linear layer\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        #self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        #self.o_proj.bias.data.fill_(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaled_dot_product(q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size() # B, N\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)  # Pass mask as a keyword argument\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        B, N, D = o.shape\n",
    "        layer_norm = nn.LayerNorm([N, D])\n",
    "        o = layer_norm(o)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9df296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom implementation:\n",
      "torch.Size([4, 20, 9])\n",
      "tensor([[[-0.4028,  1.3057,  0.9940,  0.5412, -0.8579, -0.9394,  0.5938,\n",
      "          -1.8596,  0.6290],\n",
      "         [-0.4086,  1.3101,  1.0071,  0.5427, -0.8552, -0.9456,  0.5935,\n",
      "          -1.8722,  0.6328],\n",
      "         [-0.4031,  1.3023,  0.9912,  0.5414, -0.8571, -0.9367,  0.5914,\n",
      "          -1.8563,  0.6271],\n",
      "         [-0.4067,  1.3093,  1.0045,  0.5399, -0.8552, -0.9441,  0.5932,\n",
      "          -1.8728,  0.6328],\n",
      "         [-0.4021,  1.3021,  0.9905,  0.5391, -0.8575, -0.9374,  0.5905,\n",
      "          -1.8614,  0.6293],\n",
      "         [-0.4023,  1.3066,  0.9937,  0.5415, -0.8586, -0.9399,  0.5947,\n",
      "          -1.8591,  0.6293],\n",
      "         [-0.4017,  1.3030,  0.9905,  0.5386, -0.8573, -0.9369,  0.5911,\n",
      "          -1.8603,  0.6290],\n",
      "         [-0.4095,  1.3045,  1.0025,  0.5425, -0.8535, -0.9408,  0.5881,\n",
      "          -1.8681,  0.6311],\n",
      "         [-0.4060,  1.3057,  1.0005,  0.5398, -0.8550, -0.9412,  0.5919,\n",
      "          -1.8681,  0.6300],\n",
      "         [-0.4074,  1.3094,  1.0044,  0.5411, -0.8553, -0.9439,  0.5925,\n",
      "          -1.8714,  0.6332],\n",
      "         [-0.4071,  1.3128,  1.0063,  0.5441, -0.8575, -0.9471,  0.5966,\n",
      "          -1.8700,  0.6335],\n",
      "         [-0.4031,  1.3028,  0.9917,  0.5404, -0.8568, -0.9369,  0.5910,\n",
      "          -1.8584,  0.6281],\n",
      "         [-0.4043,  1.3063,  0.9957,  0.5442, -0.8583, -0.9408,  0.5942,\n",
      "          -1.8587,  0.6291],\n",
      "         [-0.4079,  1.3073,  1.0015,  0.5451, -0.8563, -0.9426,  0.5923,\n",
      "          -1.8639,  0.6309],\n",
      "         [-0.4083,  1.3038,  0.9990,  0.5440, -0.8550, -0.9398,  0.5888,\n",
      "          -1.8629,  0.6300],\n",
      "         [-0.4005,  1.3010,  0.9877,  0.5392, -0.8582, -0.9362,  0.5919,\n",
      "          -1.8570,  0.6268],\n",
      "         [-0.4094,  1.3137,  1.0105,  0.5427, -0.8550, -0.9473,  0.5945,\n",
      "          -1.8748,  0.6352],\n",
      "         [-0.4019,  1.2976,  0.9866,  0.5382, -0.8561, -0.9334,  0.5874,\n",
      "          -1.8576,  0.6267],\n",
      "         [-0.4032,  1.3020,  0.9947,  0.5352, -0.8549, -0.9383,  0.5885,\n",
      "          -1.8709,  0.6310],\n",
      "         [-0.4055,  1.3066,  0.9997,  0.5392, -0.8553, -0.9409,  0.5913,\n",
      "          -1.8686,  0.6316]],\n",
      "\n",
      "        [[-0.5850,  1.1570,  1.1079,  0.6311, -0.6566, -0.6549,  0.3444,\n",
      "          -2.0761,  0.7437],\n",
      "         [-0.5885,  1.1356,  1.0918,  0.6347, -0.6498, -0.6377,  0.3270,\n",
      "          -2.0436,  0.7313],\n",
      "         [-0.5876,  1.1300,  1.0910,  0.6253, -0.6467, -0.6421,  0.3271,\n",
      "          -2.0596,  0.7398],\n",
      "         [-0.5865,  1.1328,  1.0951,  0.6226, -0.6414, -0.6377,  0.3196,\n",
      "          -2.0555,  0.7365],\n",
      "         [-0.5824,  1.1367,  1.0863,  0.6241, -0.6536, -0.6404,  0.3289,\n",
      "          -2.0605,  0.7370],\n",
      "         [-0.5848,  1.1372,  1.0903,  0.6252, -0.6499, -0.6385,  0.3250,\n",
      "          -2.0558,  0.7352],\n",
      "         [-0.5908,  1.1427,  1.1046,  0.6343, -0.6478, -0.6474,  0.3340,\n",
      "          -2.0583,  0.7390],\n",
      "         [-0.5887,  1.1413,  1.1047,  0.6293, -0.6441, -0.6453,  0.3293,\n",
      "          -2.0601,  0.7391],\n",
      "         [-0.5823,  1.1613,  1.1191,  0.6210, -0.6466, -0.6563,  0.3383,\n",
      "          -2.0879,  0.7479],\n",
      "         [-0.5850,  1.1498,  1.1068,  0.6226, -0.6482, -0.6500,  0.3332,\n",
      "          -2.0763,  0.7443],\n",
      "         [-0.5868,  1.1214,  1.0833,  0.6236, -0.6407, -0.6291,  0.3123,\n",
      "          -2.0408,  0.7306],\n",
      "         [-0.5866,  1.1376,  1.0934,  0.6249, -0.6504, -0.6439,  0.3296,\n",
      "          -2.0642,  0.7403],\n",
      "         [-0.5900,  1.1483,  1.1041,  0.6375, -0.6525, -0.6467,  0.3362,\n",
      "          -2.0552,  0.7361],\n",
      "         [-0.5882,  1.1555,  1.1174,  0.6315, -0.6439, -0.6488,  0.3324,\n",
      "          -2.0640,  0.7389],\n",
      "         [-0.5806,  1.1465,  1.1021,  0.6196, -0.6449, -0.6428,  0.3259,\n",
      "          -2.0684,  0.7390],\n",
      "         [-0.5820,  1.1336,  1.0832,  0.6241, -0.6531, -0.6383,  0.3272,\n",
      "          -2.0570,  0.7355],\n",
      "         [-0.5865,  1.1441,  1.1096,  0.6242, -0.6394, -0.6448,  0.3255,\n",
      "          -2.0646,  0.7401],\n",
      "         [-0.5918,  1.1577,  1.1204,  0.6399, -0.6475, -0.6530,  0.3400,\n",
      "          -2.0613,  0.7392],\n",
      "         [-0.5855,  1.1506,  1.1016,  0.6313, -0.6569, -0.6521,  0.3423,\n",
      "          -2.0712,  0.7423],\n",
      "         [-0.5872,  1.1446,  1.1070,  0.6266, -0.6415, -0.6418,  0.3240,\n",
      "          -2.0580,  0.7366]],\n",
      "\n",
      "        [[-0.4984,  1.2523,  1.0221,  0.6471, -0.7939, -0.7687,  0.4814,\n",
      "          -1.9923,  0.6709],\n",
      "         [-0.4978,  1.2385,  1.0212,  0.6439, -0.7850, -0.7676,  0.4740,\n",
      "          -1.9873,  0.6655],\n",
      "         [-0.4971,  1.2397,  1.0186,  0.6446, -0.7865, -0.7657,  0.4744,\n",
      "          -1.9848,  0.6655],\n",
      "         [-0.4901,  1.2140,  0.9804,  0.6379, -0.7889, -0.7418,  0.4561,\n",
      "          -1.9600,  0.6545],\n",
      "         [-0.5001,  1.2532,  1.0244,  0.6453, -0.7940, -0.7697,  0.4800,\n",
      "          -1.9997,  0.6734],\n",
      "         [-0.4956,  1.2351,  1.0123,  0.6444, -0.7865, -0.7618,  0.4720,\n",
      "          -1.9786,  0.6629],\n",
      "         [-0.4953,  1.2378,  1.0143,  0.6434, -0.7887, -0.7654,  0.4741,\n",
      "          -1.9822,  0.6641],\n",
      "         [-0.4957,  1.2417,  1.0166,  0.6455, -0.7888, -0.7654,  0.4763,\n",
      "          -1.9817,  0.6652],\n",
      "         [-0.4974,  1.2488,  1.0258,  0.6469, -0.7899, -0.7725,  0.4819,\n",
      "          -1.9895,  0.6683],\n",
      "         [-0.4941,  1.2363,  1.0047,  0.6431, -0.7915, -0.7580,  0.4710,\n",
      "          -1.9773,  0.6637],\n",
      "         [-0.5000,  1.2460,  1.0246,  0.6444, -0.7879, -0.7682,  0.4757,\n",
      "          -1.9954,  0.6703],\n",
      "         [-0.4969,  1.2323,  1.0111,  0.6425, -0.7846, -0.7588,  0.4675,\n",
      "          -1.9811,  0.6636],\n",
      "         [-0.4957,  1.2348,  1.0128,  0.6421, -0.7868, -0.7629,  0.4707,\n",
      "          -1.9823,  0.6639],\n",
      "         [-0.4930,  1.2226,  1.0001,  0.6401, -0.7845, -0.7545,  0.4632,\n",
      "          -1.9703,  0.6581],\n",
      "         [-0.4908,  1.2199,  0.9867,  0.6399, -0.7891, -0.7458,  0.4603,\n",
      "          -1.9626,  0.6564],\n",
      "         [-0.4955,  1.2418,  1.0147,  0.6448, -0.7901, -0.7646,  0.4757,\n",
      "          -1.9823,  0.6656],\n",
      "         [-0.4889,  1.2131,  0.9840,  0.6390, -0.7861, -0.7453,  0.4582,\n",
      "          -1.9559,  0.6524],\n",
      "         [-0.4985,  1.2446,  1.0251,  0.6460, -0.7870, -0.7702,  0.4782,\n",
      "          -1.9898,  0.6676],\n",
      "         [-0.4975,  1.2412,  1.0175,  0.6437, -0.7883, -0.7648,  0.4737,\n",
      "          -1.9877,  0.6671],\n",
      "         [-0.4933,  1.2332,  0.9996,  0.6420, -0.7924, -0.7552,  0.4688,\n",
      "          -1.9752,  0.6626]],\n",
      "\n",
      "        [[-0.6574,  1.1414,  1.1028,  0.6889, -0.6548, -0.6416,  0.3093,\n",
      "          -2.0137,  0.7190],\n",
      "         [-0.6608,  1.1421,  1.1074,  0.6877, -0.6524, -0.6415,  0.3051,\n",
      "          -2.0203,  0.7220],\n",
      "         [-0.6604,  1.1441,  1.1130,  0.6893, -0.6505, -0.6455,  0.3089,\n",
      "          -2.0223,  0.7219],\n",
      "         [-0.6590,  1.1421,  1.1071,  0.6888, -0.6539, -0.6446,  0.3100,\n",
      "          -2.0165,  0.7195],\n",
      "         [-0.6566,  1.1440,  1.1059,  0.6902, -0.6560, -0.6461,  0.3143,\n",
      "          -2.0135,  0.7185],\n",
      "         [-0.6587,  1.1484,  1.1095,  0.6896, -0.6552, -0.6440,  0.3126,\n",
      "          -2.0220,  0.7231],\n",
      "         [-0.6628,  1.1352,  1.1038,  0.6855, -0.6494, -0.6368,  0.2974,\n",
      "          -2.0189,  0.7208],\n",
      "         [-0.6634,  1.1516,  1.1201,  0.6893, -0.6549, -0.6533,  0.3127,\n",
      "          -2.0288,  0.7255],\n",
      "         [-0.6618,  1.1403,  1.1071,  0.6869, -0.6511, -0.6399,  0.3027,\n",
      "          -2.0212,  0.7221],\n",
      "         [-0.6599,  1.1559,  1.1242,  0.6922, -0.6519, -0.6530,  0.3193,\n",
      "          -2.0313,  0.7260],\n",
      "         [-0.6586,  1.1459,  1.1072,  0.6892, -0.6555, -0.6440,  0.3109,\n",
      "          -2.0189,  0.7218],\n",
      "         [-0.6600,  1.1492,  1.1192,  0.6910, -0.6501, -0.6494,  0.3145,\n",
      "          -2.0265,  0.7233],\n",
      "         [-0.6575,  1.1311,  1.0938,  0.6867, -0.6552, -0.6388,  0.3034,\n",
      "          -2.0035,  0.7138],\n",
      "         [-0.6544,  1.1521,  1.1104,  0.6929, -0.6554, -0.6455,  0.3206,\n",
      "          -2.0197,  0.7218],\n",
      "         [-0.6549,  1.1434,  1.1000,  0.6897, -0.6531, -0.6347,  0.3093,\n",
      "          -2.0154,  0.7207],\n",
      "         [-0.6631,  1.1486,  1.1165,  0.6887, -0.6542, -0.6501,  0.3096,\n",
      "          -2.0265,  0.7247],\n",
      "         [-0.6610,  1.1444,  1.1109,  0.6885, -0.6549, -0.6480,  0.3103,\n",
      "          -2.0199,  0.7211],\n",
      "         [-0.6606,  1.1559,  1.1246,  0.6920, -0.6527, -0.6543,  0.3193,\n",
      "          -2.0312,  0.7259],\n",
      "         [-0.6555,  1.1419,  1.1025,  0.6899, -0.6520, -0.6375,  0.3102,\n",
      "          -2.0150,  0.7194],\n",
      "         [-0.6581,  1.1550,  1.1207,  0.6927, -0.6541, -0.6536,  0.3214,\n",
      "          -2.0264,  0.7240]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N=20\n",
    "E=9 # is also the output dimension!\n",
    "D=5\n",
    "B=4\n",
    "\n",
    "input = torch.rand(B, N, D)\n",
    "\n",
    "custom_att = MultiheadAttention(input_dim=D, embed_dim=9, num_heads=3)\n",
    "\n",
    "Z_custom = custom_att(input)\n",
    "\n",
    "print(\"Custom implementation:\")\n",
    "print(Z_custom.shape)\n",
    "print(Z_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09f82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191a59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
