{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38449040",
   "metadata": {},
   "source": [
    "Reference: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a116afe",
   "metadata": {},
   "source": [
    "How to use: <br>\n",
    "initialize with: <br>\n",
    "input dimension D (input_dim) <br>\n",
    "output dimension E (embed_dim) <br>\n",
    "number of attention heads (num_heads) <br>\n",
    "--> embed_dim % num_heads == 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92bceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9da7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, N, input_dim, embed_dim, num_heads): # embed_dim = num_heads*D_weights! input_dim=D!\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim, bias=False) # Linear layer\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm([N, embed_dim])\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        #self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        #self.o_proj.bias.data.fill_(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaled_dot_product(q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size() # B, N\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k ,v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)  # Pass mask as a keyword argument\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        o = self.layer_norm(o)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9df296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 9])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "N=20\n",
    "E=9 # is also the output dimension!\n",
    "D=6\n",
    "B=4\n",
    "\n",
    "matrix = torch.rand(B, N, D)\n",
    "\n",
    "att = MultiheadAttention(N, input_dim=D, embed_dim=E, num_heads=3)\n",
    "\n",
    "Z = att(matrix)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09f82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191a59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
